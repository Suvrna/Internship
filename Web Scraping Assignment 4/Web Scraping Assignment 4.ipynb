{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e1da9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "#Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "#A) Rank, B) Name C) Artist D) Upload date E) Views\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "try:\n",
    "#1. First get the webpage https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "    driver = webdriver.Chrome()\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "    driver.get(url)\n",
    "\n",
    "#/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[1]/td[1]\n",
    "    noData=[]\n",
    "    vidnameData=[]\n",
    "    upData=[]\n",
    "    pubData=[]\n",
    "    viewData=[]\n",
    "\n",
    "\n",
    "#4. Then scrape the data for #A) Rank, B) Name C) Artist D) Upload date E) Views.\n",
    "#page=requests.get(url)\n",
    "#soup = BeautifulSoup(page.content)\n",
    "\n",
    "    for i in range(1,32):\n",
    "        pth=\"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[\"+str(i)+\"]/\"\n",
    "        alist=driver.find_elements(By.XPATH,pth+\"td[1]\")\n",
    "        vidlist=driver.find_elements(By.XPATH,pth+\"td[2]\")\n",
    "        uplist=driver.find_elements(By.XPATH,pth+\"td[3]\")\n",
    "        vlist=driver.find_elements(By.XPATH,pth+\"td[4]\")\n",
    "        plist=driver.find_elements(By.XPATH,pth+\"td[5]\")\n",
    "    \n",
    "        for k in alist:\n",
    "            noData.append(k.text)\n",
    "    \n",
    "        for l in vidlist:\n",
    "            vidnameData.append(l.text)\n",
    "                                 \n",
    "        for m in uplist:\n",
    "            upData.append(m.text)\n",
    "    \n",
    "        for n in vlist:\n",
    "            viewData.append(n.text)   \n",
    "\n",
    "        for o in plist:\n",
    "            pubData.append(o.text)\n",
    "        \n",
    "        \n",
    "#5. Finally create a dataframe of the scraped data.#A) Rank, B) Name C) Artist D) Upload date E) Views\n",
    "    dfVideos= pd.DataFrame({'Rank':noData[:31],'Name':vidnameData[:31],'Artist':upData[:31], 'Upload date':pubData[:31], 'Views':viewData[:31]})\n",
    "    dfVideos\n",
    "\n",
    "    #close browser\n",
    "   # driver.close()\n",
    "    \n",
    "    # some error is occ        \n",
    "except Exception as e:\n",
    "    print(\"Error raised by System: \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e82ac006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[16]</td>\n",
       "      <td>LooLoo Kids - Nursery Rhymes and Children's Songs</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[18]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[21]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>6.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[27]</td>\n",
       "      <td>ChuChu TV Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[28]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors ‚Äì Colorful Eggs on a Farm\"[29]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[30]</td>\n",
       "      <td>officialpsy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear ‚Äì Recipe for Disaster\"[35]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[36]</td>\n",
       "      <td>Ultra Records</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[38]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[39]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Roar\"[40]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[42]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Sorry\"[43]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[44]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Thinking Out Loud\"[45]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Dark Horse\"[46]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[47]</td>\n",
       "      <td>Kiddiestv Hindi - Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Perfect\"[48]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Faded\"[49]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Let Her Go\"[50]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Girls Like You\"[51]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Lean On\"[52]</td>\n",
       "      <td>Major Lazer Official</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Bailando\"[53]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[6]   \n",
       "1    2.                                   \"Despacito\"[9]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[16]   \n",
       "3    4.                                  \"Bath Song\"[17]   \n",
       "4    5.                               \"Shape of You\"[18]   \n",
       "5    6.                              \"See You Again\"[21]   \n",
       "6    7.                          \"Wheels on the Bus\"[26]   \n",
       "7    8.                \"Phonics Song with Two Words\"[27]   \n",
       "8    9.                                \"Uptown Funk\"[28]   \n",
       "9   10.  \"Learning Colors ‚Äì Colorful Eggs on a Farm\"[29]   \n",
       "10  11.                              \"Gangnam Style\"[30]   \n",
       "11  12.   \"Masha and the Bear ‚Äì Recipe for Disaster\"[35]   \n",
       "12  13.                             \"Dame Tu Cosita\"[36]   \n",
       "13  14.                                     \"Axel F\"[37]   \n",
       "14  15.                                      \"Sugar\"[38]   \n",
       "15  16.                             \"Counting Stars\"[39]   \n",
       "16  17.                                       \"Roar\"[40]   \n",
       "17  18.                        \"Baa Baa Black Sheep\"[41]   \n",
       "18  19.           \"Waka Waka (This Time for Africa)\"[42]   \n",
       "19  20.                                      \"Sorry\"[43]   \n",
       "20  21.                             \"Lakdi Ki Kathi\"[44]   \n",
       "21  22.                          \"Thinking Out Loud\"[45]   \n",
       "22  23.                                 \"Dark Horse\"[46]   \n",
       "23  24.          \"Humpty the train on a fruits ride\"[47]   \n",
       "24  25.                                    \"Perfect\"[48]   \n",
       "25  26.                                      \"Faded\"[49]   \n",
       "26  27.                                 \"Let Her Go\"[50]   \n",
       "27  28.                             \"Girls Like You\"[51]   \n",
       "28  29.                                    \"Lean On\"[52]   \n",
       "29  30.                                   \"Bailando\"[53]   \n",
       "\n",
       "                                               Artist        Upload date  \\\n",
       "0         Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016   \n",
       "1                                          Luis Fonsi   January 12, 2017   \n",
       "2   LooLoo Kids - Nursery Rhymes and Children's Songs    October 8, 2016   \n",
       "3                          Cocomelon - Nursery Rhymes        May 2, 2018   \n",
       "4                                          Ed Sheeran   January 30, 2017   \n",
       "5                                         Wiz Khalifa      April 6, 2015   \n",
       "6                          Cocomelon - Nursery Rhymes       May 24, 2018   \n",
       "7               ChuChu TV Nursery Rhymes & Kids Songs      March 6, 2014   \n",
       "8                                         Mark Ronson  November 19, 2014   \n",
       "9                                         Miroshka TV  February 27, 2018   \n",
       "10                                        officialpsy      July 15, 2012   \n",
       "11                                         Get Movies   January 31, 2012   \n",
       "12                                      Ultra Records      April 5, 2018   \n",
       "13                                         Crazy Frog      June 16, 2009   \n",
       "14                                           Maroon 5   January 14, 2015   \n",
       "15                                        OneRepublic       May 31, 2013   \n",
       "16                                         Katy Perry  September 5, 2013   \n",
       "17                         Cocomelon - Nursery Rhymes      June 25, 2018   \n",
       "18                                            Shakira       June 4, 2010   \n",
       "19                                      Justin Bieber   October 22, 2015   \n",
       "20                                       Jingle Toons      June 14, 2018   \n",
       "21                                         Ed Sheeran    October 7, 2014   \n",
       "22                                         Katy Perry  February 20, 2014   \n",
       "23      Kiddiestv Hindi - Nursery Rhymes & Kids Songs   January 26, 2018   \n",
       "24                                         Ed Sheeran   November 9, 2017   \n",
       "25                                        Alan Walker   December 3, 2015   \n",
       "26                                          Passenger      July 25, 2012   \n",
       "27                                           Maroon 5       May 31, 2018   \n",
       "28                               Major Lazer Official     March 22, 2015   \n",
       "29                                   Enrique Iglesias     April 11, 2014   \n",
       "\n",
       "    Views  \n",
       "0   13.29  \n",
       "1    8.25  \n",
       "2    6.78  \n",
       "3    6.38  \n",
       "4    6.07  \n",
       "5    6.01  \n",
       "6    5.53  \n",
       "7    5.47  \n",
       "8    5.02  \n",
       "9    4.96  \n",
       "10   4.88  \n",
       "11   4.56  \n",
       "12   4.43  \n",
       "13   4.04  \n",
       "14   3.93  \n",
       "15   3.86  \n",
       "16   3.86  \n",
       "17   3.76  \n",
       "18   3.71  \n",
       "19   3.70  \n",
       "20   3.67  \n",
       "21   3.65  \n",
       "22   3.58  \n",
       "23   3.54  \n",
       "24   3.53  \n",
       "25   3.51  \n",
       "26   3.50  \n",
       "27   3.47  \n",
       "28   3.45  \n",
       "29   3.45  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "#Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "#A) Rank, B) Name C) Artist D) Upload date E) Views\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)\n",
    "\n",
    "#/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[1]/td[1]\n",
    "noData=[]\n",
    "vidnameData=[]\n",
    "upData=[]\n",
    "pubData=[]\n",
    "viewData=[]\n",
    "#cmpnyData=[]\n",
    "\n",
    "#4. Then scrape the data for #A) Rank, B) Name C) Artist D) Upload date E) Views.\n",
    "#page=requests.get(url)\n",
    "#soup = BeautifulSoup(page.content)\n",
    "\n",
    "for i in range(1,32):\n",
    "    pth=\"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[\"+str(i)+\"]/\"\n",
    "    alist=driver.find_elements(By.XPATH,pth+\"td[1]\")\n",
    "    vidlist=driver.find_elements(By.XPATH,pth+\"td[2]\")\n",
    "    uplist=driver.find_elements(By.XPATH,pth+\"td[3]\")\n",
    "    vlist=driver.find_elements(By.XPATH,pth+\"td[4]\")\n",
    "    plist=driver.find_elements(By.XPATH,pth+\"td[5]\")\n",
    "    \n",
    "    for k in alist:\n",
    "        noData.append(k.text)\n",
    "    \n",
    "    for l in vidlist:\n",
    "        vidnameData.append(l.text)\n",
    "                                 \n",
    "    for m in uplist:\n",
    "        upData.append(m.text)\n",
    "    \n",
    "    for n in vlist:\n",
    "        viewData.append(n.text)   \n",
    "\n",
    "    for o in plist:\n",
    "        pubData.append(o.text)\n",
    "            \n",
    "#5. Finally create a dataframe of the scraped data.#A) Rank, B) Name C) Artist D) Upload date E) Views\n",
    "dfVideos= pd.DataFrame({'Rank':noData[:31],'Name':vidnameData[:31],'Artist':upData[:31], 'Upload date':pubData[:31], 'Views':viewData[:31]})\n",
    "dfVideos\n",
    "\n",
    "#close browser\n",
    "# driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed12e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19TH ASIAN GAMES 2023</td>\n",
       "      <td>Final -</td>\n",
       "      <td>Colombo</td>\n",
       "      <td>17 SEP 2023\\n3:00 PM IST</td>\n",
       "      <td>6:30 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>Hangzhou</td>\n",
       "      <td></td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>Mohali</td>\n",
       "      <td>21 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>Indore</td>\n",
       "      <td>22 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICC MENS WORLD CUP 2023 WARM-UP MATCHES</td>\n",
       "      <td>3rd ODI -</td>\n",
       "      <td>Rajkot</td>\n",
       "      <td>24 SEP 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19TH ASIAN GAMES 2023</td>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>Guwahati</td>\n",
       "      <td>27 SEP 2023</td>\n",
       "      <td>6:30 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ICC MENS WORLD CUP 2023 WARM-UP MATCHES</td>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>Hangzhou</td>\n",
       "      <td>30 SEP 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Match title      Series     Place  \\\n",
       "0                    19TH ASIAN GAMES 2023     Final -   Colombo   \n",
       "1          AUSTRALIA TOUR OF INDIA 2023-24  1st T20I -  Hangzhou   \n",
       "2          AUSTRALIA TOUR OF INDIA 2023-24   1st ODI -    Mohali   \n",
       "3          AUSTRALIA TOUR OF INDIA 2023-24   2nd ODI -    Indore   \n",
       "4  ICC MENS WORLD CUP 2023 WARM-UP MATCHES   3rd ODI -    Rajkot   \n",
       "5                    19TH ASIAN GAMES 2023   1st ODI -  Guwahati   \n",
       "6  ICC MENS WORLD CUP 2023 WARM-UP MATCHES  1st T20I -  Hangzhou   \n",
       "\n",
       "                       Date         Time  \n",
       "0  17 SEP 2023\\n3:00 PM IST  6:30 AM IST  \n",
       "1                            1:30 PM IST  \n",
       "2               21 SEP 2023  1:30 PM IST  \n",
       "3               22 SEP 2023  1:30 PM IST  \n",
       "4               24 SEP 2023  2:00 PM IST  \n",
       "5               27 SEP 2023  6:30 AM IST  \n",
       "6               30 SEP 2023  2:00 PM IST  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2. #Scrape the details team India‚Äôs international fixtures from bcci.tv.\n",
    "#Url = https://www.bcci.tv/.\n",
    "#You need to find following details:\n",
    "#A) Match title (I.e. 1 ODI) B) Series C) Place D) Date E) Time\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver.get(url)\n",
    "\n",
    "search_btn = driver.find_element(By.XPATH,\"/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a\")\n",
    "search_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(2) # seconds\n",
    "\n",
    "titleData=[]\n",
    "dateData=[]\n",
    "timeData=[]\n",
    "testData=[]\n",
    "staData=[]\n",
    "cityData=[]\n",
    "matchData=[]\n",
    "\n",
    "    \n",
    "for i in range(1,9):\n",
    "    pth=\"/html/body/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[\"+str(i)+\"]\"\n",
    "    alist=driver.find_elements(By.XPATH, pth +\"/div/div[1]/h5\")\n",
    "    dlist=driver.find_elements(By.XPATH, pth +\"/div/div[1]/div/div[1]\")\n",
    "    tlist=driver.find_elements(By.XPATH, pth +\"/div/div[1]/div/div[2]\")\n",
    "    tstlist=driver.find_elements(By.XPATH, pth +\"/div/div[3]/div/span[1]\")\n",
    "    stlist=driver.find_elements(By.XPATH, pth +\"/div/div[3]/div/span[2]\")\n",
    "    ctlist=driver.find_elements(By.XPATH, pth +\"/div/div[3]/div/span[3]\")\n",
    "    matlist=driver.find_elements(By.XPATH, pth +\"/div/div[2]/div[1]\")\n",
    "\n",
    "    for k in alist:\n",
    "        titleData.append(k.text)\n",
    "    \n",
    "    for l in dlist:\n",
    "        dateData.append(l.text)\n",
    "\n",
    "    for m in tlist:\n",
    "        timeData.append(m.text)\n",
    "\n",
    "    for n in tstlist:\n",
    "        testData.append(n.text)\n",
    "\n",
    "    for o in stlist:\n",
    "        staData.append(o.text)\n",
    "\n",
    "    for p in ctlist:\n",
    "        cityData.append(p.text)\n",
    "\n",
    "    for q in matlist:\n",
    "        matchData.append(q.text.replace(\"\\n\",\" \"))\n",
    "\n",
    "#print(titleData)\n",
    "#print(testData)\n",
    "#print(matchData)\n",
    "#print(staData)\n",
    "#print(cityData)\n",
    "#print(dateData)\n",
    "#print(timeData)\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.#A) Match title (I.e. 1 ODI) B) Series C) Place D) Date E) Time\n",
    "dfTeam= pd.DataFrame({'Match title':titleData[:7],'Series':testData[:7],'Place':cityData[:7], 'Date':dateData[:7], 'Time':timeData[:7]})\n",
    "dfTeam\n",
    "\n",
    "#close browser\n",
    "# driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "27705b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(18-19)- at current prices</th>\n",
       "      <th>GSDP(19-20)- at current prices</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank              State GSDP(18-19)- at current prices  \\\n",
       "0     1        Maharashtra                              -   \n",
       "1     2         Tamil Nadu                      1,845,853   \n",
       "2     3      Uttar Pradesh                      1,687,818   \n",
       "3     4            Gujarat                              -   \n",
       "4     5          Karnataka                      1,631,977   \n",
       "5     6        West Bengal                      1,253,832   \n",
       "6     7          Rajasthan                      1,020,989   \n",
       "7     8     Andhra Pradesh                        972,782   \n",
       "8     9          Telangana                        969,604   \n",
       "9    10     Madhya Pradesh                        906,672   \n",
       "10   11             Kerala                              -   \n",
       "11   12              Delhi                        856,112   \n",
       "12   13            Haryana                        831,610   \n",
       "13   14              Bihar                        611,804   \n",
       "14   15             Punjab                        574,760   \n",
       "15   16             Odisha                        521,275   \n",
       "16   17              Assam                              -   \n",
       "17   18       Chhattisgarh                        329,180   \n",
       "18   19          Jharkhand                        328,598   \n",
       "19   20        Uttarakhand                              -   \n",
       "20   21    Jammu & Kashmir                              -   \n",
       "21   22   Himachal Pradesh                        165,472   \n",
       "22   23                Goa                         80,449   \n",
       "23   24            Tripura                         55,984   \n",
       "24   25         Chandigarh                              -   \n",
       "25   26         Puducherry                         38,253   \n",
       "26   27          Meghalaya                         36,572   \n",
       "27   28             Sikkim                         32,496   \n",
       "28   29            Manipur                         31,790   \n",
       "29   30           Nagaland                              -   \n",
       "30   31  Arunachal Pradesh                              -   \n",
       "\n",
       "   GSDP(19-20)- at current prices Share(18-19) GDP($ billion)  \n",
       "0                       2,632,792       13.94%        399.921  \n",
       "1                       1,630,208        8.63%        247.629  \n",
       "2                       1,584,764        8.39%        240.726  \n",
       "3                       1,502,899        7.96%        228.290  \n",
       "4                       1,493,127        7.91%        226.806  \n",
       "5                       1,089,898        5.77%        165.556  \n",
       "6                         942,586        4.99%        143.179  \n",
       "7                         862,957        4.57%        131.083  \n",
       "8                         861,031        4.56%        130.791  \n",
       "9                         809,592        4.29%        122.977  \n",
       "10                        781,653        4.14%        118.733  \n",
       "11                        774,870        4.10%        117.703  \n",
       "12                        734,163        3.89%        111.519  \n",
       "13                        530,363        2.81%         80.562  \n",
       "14                        526,376        2.79%         79.957  \n",
       "15                        487,805        2.58%         74.098  \n",
       "16                        315,881        1.67%         47.982  \n",
       "17                        304,063        1.61%         46.187  \n",
       "18                        297,204        1.57%         45.145  \n",
       "19                        245,895        1.30%         37.351  \n",
       "20                        155,956        0.83%         23.690  \n",
       "21                        153,845        0.81%         23.369  \n",
       "22                         73,170        0.39%         11.115  \n",
       "23                         49,845        0.26%          7.571  \n",
       "24                         42,114        0.22%          6.397  \n",
       "25                         34,433        0.18%          5.230  \n",
       "26                         33,481        0.18%          5.086  \n",
       "27                         28,723        0.15%          4.363  \n",
       "28                         27,870        0.15%          4.233  \n",
       "29                         27,283        0.14%          4.144  \n",
       "30                         24,603        0.13%          3.737  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "#Url = http://statisticstimes.com/\n",
    "#You have to find following details: \n",
    "#A) Rank B) State C) GSDP(18-19)- at current prices D) GSDP(19-20)- at current prices E) Share(18-19) F) GDP($ billion)\n",
    "#Note: - From statisticstimes home page you have to reach to economy page through code.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage http://statisticstimes.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "search1_btn = driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/button\")\n",
    "search1_btn.click()\n",
    "\n",
    "search2_btn = driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]\")\n",
    "search2_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "search3_btn = driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "search3_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(2) # seconds\n",
    "\n",
    "#A) Rank B) State C) GSDP(18-19)- at current prices D) GSDP(19-20)- at current prices E) Share(18-19) F) GDP($ billion)\n",
    "rankData=[]\n",
    "stateData=[]\n",
    "gsdp1Data=[]\n",
    "gsdp2Data=[]\n",
    "shareData=[]\n",
    "gdpMData=[]\n",
    "\n",
    "for i in range(1,34):\n",
    "    pth=\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[\"+str(i)+\"]/\"\n",
    "    rlist=driver.find_elements(By.XPATH,pth+\"td[1]\")\n",
    "    stlist=driver.find_elements(By.XPATH,pth+\"td[2]\")\n",
    "    g1list=driver.find_elements(By.XPATH,pth+\"td[3]\")\n",
    "    g2list=driver.find_elements(By.XPATH,pth+\"td[4]\")\n",
    "    shlist=driver.find_elements(By.XPATH,pth+\"td[5]\")\n",
    "    gdlist=driver.find_elements(By.XPATH,pth+\"td[6]\")\n",
    "    \n",
    "    for k in rlist:\n",
    "        rankData.append(k.text)\n",
    "    \n",
    "    for l in stlist:\n",
    "        stateData.append(l.text)\n",
    "                                 \n",
    "    for m in g1list:\n",
    "        gsdp1Data.append(m.text)\n",
    "    \n",
    "    for n in g2list:\n",
    "        gsdp2Data.append(n.text)   \n",
    "\n",
    "    for o in shlist:\n",
    "        shareData.append(o.text)\n",
    "\n",
    "    for p in gdlist:\n",
    "        gdpMData.append(p.text)\n",
    "\n",
    "#print(rankData)\n",
    "#print(stateData)\n",
    "#print(gsdp1Data)\n",
    "#print(gsdp2Data)\n",
    "#print(shareData)\n",
    "#print(gdpMData)\n",
    "\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.#A) Rank B) State C) GSDP(18-19)- at current prices D) GSDP(19-20)- at current prices E) Share(18-19) F) GDP($ billion)\n",
    "dfGDPState= pd.DataFrame({'Rank':rankData[:31],'State':stateData[:31],'GSDP(18-19)- at current prices':gsdp1Data[:31], 'GSDP(19-20)- at current prices':gsdp2Data[:31], 'Share(18-19)':shareData[:31],'GDP($ billion)':gdpMData[:31]})\n",
    "dfGDPState\n",
    "\n",
    "#close browser\n",
    "# driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ccee51c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository title</th>\n",
       "      <th>Repository description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coqui-ai /</td>\n",
       "      <td>üê∏üí¨ - a deep learning toolkit for Text-to-Speec...</td>\n",
       "      <td>1,970</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aiwaves-cn /</td>\n",
       "      <td>An Open-source Framework for Autonomous Langua...</td>\n",
       "      <td>32</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>isocpp /</td>\n",
       "      <td>The C++ Core Guidelines are a set of tried-and...</td>\n",
       "      <td>5,275</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>godotengine /</td>\n",
       "      <td>Godot Engine ‚Äì Multi-platform 2D and 3D game e...</td>\n",
       "      <td>14,095</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>godotengine /</td>\n",
       "      <td>Godot Engine official documentation</td>\n",
       "      <td>2,521</td>\n",
       "      <td>reStructuredText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FlaxEngine /</td>\n",
       "      <td>Flax Engine ‚Äì multi-platform 3D game engine</td>\n",
       "      <td>436</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yoheinakajima /</td>\n",
       "      <td>Converts text input or URL into knowledge grap...</td>\n",
       "      <td>186</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stride3d /</td>\n",
       "      <td>Stride Game Engine (formerly Xenko)</td>\n",
       "      <td>791</td>\n",
       "      <td>C#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nuejs /</td>\n",
       "      <td>Build user interfaces with 10x less code. Alte...</td>\n",
       "      <td>20</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>raysan5 /</td>\n",
       "      <td>A simple and easy-to-use library to enjoy vide...</td>\n",
       "      <td>1,679</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>godotengine /</td>\n",
       "      <td>C++ bindings for the Godot script API</td>\n",
       "      <td>376</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sxyazi /</td>\n",
       "      <td>‚ö°Ô∏è Blazing fast terminal file manager written ...</td>\n",
       "      <td>26</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lodash /</td>\n",
       "      <td>A modern JavaScript utility library delivering...</td>\n",
       "      <td>7,071</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TheCherno /</td>\n",
       "      <td>Hazel Engine</td>\n",
       "      <td>1,409</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kholia /</td>\n",
       "      <td>Run macOS on QEMU/KVM. With OpenCore + Big Sur...</td>\n",
       "      <td>1,659</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tldraw /</td>\n",
       "      <td>a very good whiteboard</td>\n",
       "      <td>1,103</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bevyengine /</td>\n",
       "      <td>A refreshingly simple data-driven game engine ...</td>\n",
       "      <td>2,607</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DroidKaigi /</td>\n",
       "      <td>The Official Conference App for DroidKaigi 2023</td>\n",
       "      <td>193</td>\n",
       "      <td>Kotlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>godotengine /</td>\n",
       "      <td>Demonstration and Template Projects</td>\n",
       "      <td>1,331</td>\n",
       "      <td>GDScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MarlinFirmware /</td>\n",
       "      <td>Marlin is an optimized firmware for RepRap 3D ...</td>\n",
       "      <td>18,598</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ethereum-lists /</td>\n",
       "      <td>provides metadata for chains</td>\n",
       "      <td>5,208</td>\n",
       "      <td>Kotlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>commaai /</td>\n",
       "      <td>openpilot is an open source driver assistance ...</td>\n",
       "      <td>7,635</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>public-apis /</td>\n",
       "      <td>A collective list of free APIs</td>\n",
       "      <td>29,197</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Kr328 /</td>\n",
       "      <td>A rule-based tunnel for Android.</td>\n",
       "      <td>2,992</td>\n",
       "      <td>Kotlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JetBrains /</td>\n",
       "      <td>Compose Multiplatform, a modern UI framework f...</td>\n",
       "      <td>972</td>\n",
       "      <td>Kotlin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Repository title                             Repository description  \\\n",
       "0         coqui-ai /  üê∏üí¨ - a deep learning toolkit for Text-to-Speec...   \n",
       "1       aiwaves-cn /  An Open-source Framework for Autonomous Langua...   \n",
       "2           isocpp /  The C++ Core Guidelines are a set of tried-and...   \n",
       "3      godotengine /  Godot Engine ‚Äì Multi-platform 2D and 3D game e...   \n",
       "4      godotengine /                Godot Engine official documentation   \n",
       "5       FlaxEngine /        Flax Engine ‚Äì multi-platform 3D game engine   \n",
       "6    yoheinakajima /  Converts text input or URL into knowledge grap...   \n",
       "7         stride3d /                Stride Game Engine (formerly Xenko)   \n",
       "8            nuejs /  Build user interfaces with 10x less code. Alte...   \n",
       "9          raysan5 /  A simple and easy-to-use library to enjoy vide...   \n",
       "10     godotengine /              C++ bindings for the Godot script API   \n",
       "11          sxyazi /  ‚ö°Ô∏è Blazing fast terminal file manager written ...   \n",
       "12          lodash /  A modern JavaScript utility library delivering...   \n",
       "13       TheCherno /                                       Hazel Engine   \n",
       "14          kholia /  Run macOS on QEMU/KVM. With OpenCore + Big Sur...   \n",
       "15          tldraw /                             a very good whiteboard   \n",
       "16      bevyengine /  A refreshingly simple data-driven game engine ...   \n",
       "17      DroidKaigi /    The Official Conference App for DroidKaigi 2023   \n",
       "18     godotengine /                Demonstration and Template Projects   \n",
       "19  MarlinFirmware /  Marlin is an optimized firmware for RepRap 3D ...   \n",
       "20  ethereum-lists /                       provides metadata for chains   \n",
       "21         commaai /  openpilot is an open source driver assistance ...   \n",
       "22     public-apis /                     A collective list of free APIs   \n",
       "23           Kr328 /                   A rule-based tunnel for Android.   \n",
       "24       JetBrains /  Compose Multiplatform, a modern UI framework f...   \n",
       "\n",
       "   Contributors count     Language used  \n",
       "0               1,970            Python  \n",
       "1                  32            Python  \n",
       "2               5,275            Python  \n",
       "3              14,095               C++  \n",
       "4               2,521  reStructuredText  \n",
       "5                 436               C++  \n",
       "6                 186            Python  \n",
       "7                 791                C#  \n",
       "8                  20        JavaScript  \n",
       "9               1,679                 C  \n",
       "10                376               C++  \n",
       "11                 26              Rust  \n",
       "12              7,071        JavaScript  \n",
       "13              1,409               C++  \n",
       "14              1,659            Python  \n",
       "15              1,103        TypeScript  \n",
       "16              2,607              Rust  \n",
       "17                193            Kotlin  \n",
       "18              1,331          GDScript  \n",
       "19             18,598               C++  \n",
       "20              5,208            Kotlin  \n",
       "21              7,635            Python  \n",
       "22             29,197            Python  \n",
       "23              2,992            Kotlin  \n",
       "24                972            Kotlin  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Scrape the details of trending repositories on Github.com.\n",
    "#Url = https://github.com/\n",
    "#You have to find the following details:\n",
    "#A) Repository title #B) Repository description #C) Contributors count #D) Language used\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage https://github.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://github.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "search1_btn = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button\")\n",
    "search1_btn.click()\n",
    "\n",
    "search2_btn = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a\")\n",
    "search2_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "#A) Repository title #B) Repository description #C) Contributors count #D) Language used\n",
    "titleData=[]\n",
    "descData=[]\n",
    "countData=[]\n",
    "langData=[]\n",
    "\n",
    "for i in range(1,26):\n",
    "    pth=\"/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article[\"+str(i)+\"]\"\n",
    "    tltlist=driver.find_elements(By.XPATH, pth + \"/h2/a/span\")\n",
    "    destlist=driver.find_elements(By.XPATH,pth + \"/p\")\n",
    "    cntlist=driver.find_elements(By.XPATH,pth + \"/div[2]/a[2]\")\n",
    "    lanlist=driver.find_elements(By.XPATH,pth + \"/div[2]/span[1]/span[2]\")\n",
    "\n",
    "    for k in tltlist:\n",
    "        titleData.append(k.text)\n",
    "    \n",
    "    for l in destlist:\n",
    "        descData.append(l.text)\n",
    "                                 \n",
    "    for m in cntlist:\n",
    "        countData.append(m.text)\n",
    "    \n",
    "    for n in lanlist:\n",
    "        langData.append(n.text)   \n",
    "\n",
    "#print(titleData)\n",
    "#print(descData)\n",
    "#print(countData)\n",
    "#print(langData)\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.#A) Repository title #B) Repository description #C) Contributors count #D) Language used\n",
    "dfGitData= pd.DataFrame({'Repository title':titleData[:25],'Repository description':descData[:25],'Contributors count':countData[:25], 'Language used':langData[:25]})\n",
    "dfGitData\n",
    "\n",
    "#close browser\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1909a86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song name</th>\n",
       "      <th>Artist name</th>\n",
       "      <th>Last week rank</th>\n",
       "      <th>Peak rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paint The Town Red</td>\n",
       "      <td>Doja Cat</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I Remember Everything</td>\n",
       "      <td>Zach Bryan Featuring Kacey Musgraves</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Sittin' On Top Of The World</td>\n",
       "      <td>Burna Boy</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Pretty Little Poison</td>\n",
       "      <td>Warren Zeiders</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Oklahoman Son</td>\n",
       "      <td>Zach Bryan</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>TQM</td>\n",
       "      <td>Fuerza Regida</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>See You Again</td>\n",
       "      <td>Tyler, The Creator Featuring Kali Uchis</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Song name                              Artist name  \\\n",
       "0            Paint The Town Red                                 Doja Cat   \n",
       "1         I Remember Everything     Zach Bryan Featuring Kacey Musgraves   \n",
       "2                      Fast Car                               Luke Combs   \n",
       "3                  Cruel Summer                             Taylor Swift   \n",
       "4                    Last Night                            Morgan Wallen   \n",
       "..                          ...                                      ...   \n",
       "85  Sittin' On Top Of The World                                Burna Boy   \n",
       "86         Pretty Little Poison                           Warren Zeiders   \n",
       "87                Oklahoman Son                               Zach Bryan   \n",
       "88                          TQM                            Fuerza Regida   \n",
       "89                See You Again  Tyler, The Creator Featuring Kali Uchis   \n",
       "\n",
       "   Last week rank Peak rank Weeks on board  \n",
       "0               3         3              5  \n",
       "1               1         1              2  \n",
       "2               2         2             24  \n",
       "3               5         5             18  \n",
       "4               4         4             32  \n",
       "..            ...       ...            ...  \n",
       "85             80        80              2  \n",
       "86             92        92              4  \n",
       "87             47        47              2  \n",
       "88             91        91             16  \n",
       "89             87        87             20  \n",
       "\n",
       "[90 rows x 5 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/\n",
    "#You have to find the following details:\n",
    "#A) Song name B) Artist name C) Last week rank D) Peak rank E) Weeks on board\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage https://github.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https:/www.billboard.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "chart_btn = driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a\")\n",
    "chart_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "bHun_btn = driver.find_element(By.XPATH,\"/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div/div[2]/span/a\")\n",
    "bHun_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#A) Song name B) Artist name C) Last week rank D) Peak rank E) Weeks on board\n",
    "songData=[]\n",
    "artistData=[]\n",
    "lstwekData=[]\n",
    "pekrankData=[]\n",
    "wobData=[]\n",
    "\n",
    "for i in range(1,101):\n",
    "    pth=\"/html/body/div[4]/main/div[2]/div[3]/div/div/div/div[2]/div[\"+str(i)+\"]\"\n",
    "    snglist=driver.find_elements(By.XPATH, pth + \"/ul/li[4]/ul/li[1]/h3\")\n",
    "    artlist=driver.find_elements(By.XPATH,pth + \"/ul/li[4]/ul/li[1]/span\")\n",
    "    lwrlist=driver.find_elements(By.XPATH,pth + \"/ul/li[4]/ul/li[4]/span\")\n",
    "    pkrlist=driver.find_elements(By.XPATH,pth + \"/ul/li[4]/ul/li[4]/span\")\n",
    "    woblist=driver.find_elements(By.XPATH,pth + \"/ul/li[4]/ul/li[6]/span\")\n",
    "\n",
    "    for k in snglist:\n",
    "        songData.append(k.text)\n",
    "    \n",
    "    for l in artlist:\n",
    "        artistData.append(l.text)\n",
    "                                 \n",
    "    for m in lwrlist:\n",
    "        lstwekData.append(m.text)\n",
    "    \n",
    "    for n in pkrlist:\n",
    "        pekrankData.append(n.text)     \n",
    "\n",
    "    for o in woblist:\n",
    "        wobData.append(o.text) \n",
    "        \n",
    "#print(songData)\n",
    "#print(artistData)\n",
    "#print(lstwekData)\n",
    "#print(pekrankData)\n",
    "#print(wobData)\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data. #A) Song name B) Artist name C) Last week rank D) Peak rank E) Weeks on board\n",
    "dfSongData= pd.DataFrame({'Song name':songData[:100],'Artist name':artistData[:100],'Last week rank':lstwekData[:100], 'Peak rank':pekrankData[:100], 'Weeks on board':wobData[:100]})\n",
    "dfSongData\n",
    "\n",
    "#close browser\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "942cdca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Book name</th>\n",
       "      <th>Author name</th>\n",
       "      <th>Volumes sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                          Book name       Author name  \\\n",
       "0     1                                  Da Vinci Code,The        Brown, Dan   \n",
       "1     2               Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2     3           Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3     4          Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4     5                               Fifty Shades of Grey      James, E. L.   \n",
       "..  ...                                                ...               ...   \n",
       "95   96                                          Ghost,The    Harris, Robert   \n",
       "96   97                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97   98              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98   99  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  100  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. Scrape the details of Highest selling novels.\n",
    "# Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-\n",
    "#https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "#You have to find the following details:\n",
    "#compare\n",
    "#A) Book name #B) Author name #C) Volumes sold #D) Publisher #E) Genre\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage https://github.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver.get(url)\n",
    "\n",
    "#A) Book name #B) Author name #C) Volumes sold #D) Publisher #E) Genre\n",
    "rankData=[]\n",
    "titleData=[]\n",
    "authData=[]\n",
    "volsaleData=[]\n",
    "pubData=[]\n",
    "genreData=[]\n",
    "\n",
    "for i in range(1,101):\n",
    "    pth=\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[\"+str(i)+\"]\"\n",
    "    rlist=driver.find_elements(By.XPATH, pth + \"/td[1]\")\n",
    "    tlist=driver.find_elements(By.XPATH,pth + \"/td[2]\")\n",
    "    alist=driver.find_elements(By.XPATH,pth + \"/td[3]\")\n",
    "    vslist=driver.find_elements(By.XPATH,pth + \"/td[4]\")\n",
    "    plist=driver.find_elements(By.XPATH,pth + \"/td[5]\")\n",
    "    genlist=driver.find_elements(By.XPATH,pth + \"/td[6]\")\n",
    "\n",
    "    for k in rlist:\n",
    "        rankData.append(k.text)\n",
    "    \n",
    "    for l in tlist:\n",
    "        titleData.append(l.text)\n",
    "                                 \n",
    "    for m in alist:\n",
    "        authData.append(m.text)\n",
    "    \n",
    "    for n in vslist:\n",
    "        volsaleData.append(n.text)     \n",
    "\n",
    "    for o in plist:\n",
    "        pubData.append(o.text)\n",
    "\n",
    "    for p in genlist:\n",
    "        genreData.append(p.text)\n",
    "        \n",
    "\n",
    "#5. Finally create a dataframe of the scraped data. #A) Book name #B) Author name #C) Volumes sold #D) Publisher #E) Genre\n",
    "dfBookData= pd.DataFrame({'Rank':rankData[:100],'Book name':titleData[:100],'Author name':authData[:100], 'Volumes sold':volsaleData[:100], 'Publisher':pubData[:100], 'Genre':genreData[:100]})\n",
    "dfBookData\n",
    "\n",
    "#close browser\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "62e1d775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year span</th>\n",
       "      <th>Run time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011‚Äì2019)</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,204,657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016‚Äì2025)</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,276,113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010‚Äì2022)</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,046,066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017‚Äì2020)</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>307,424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014‚Äì2020)</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>266,508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013‚Äì2017)</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>52,738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017‚Äì2019)</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>64,797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005‚Äì )</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>211,076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015‚Äì2019)</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>43,908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>266,096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year span Run time Ratings      Votes\n",
       "0                  Game of Thrones  (2011‚Äì2019)   57 min     9.2  2,204,657\n",
       "1                  Stranger Things  (2016‚Äì2025)   51 min     8.7  1,276,113\n",
       "2                 The Walking Dead  (2010‚Äì2022)   44 min     8.1  1,046,066\n",
       "3                   13 Reasons Why  (2017‚Äì2020)   60 min     7.5    307,424\n",
       "4                          The 100  (2014‚Äì2020)   43 min     7.6    266,508\n",
       "..                             ...          ...      ...     ...        ...\n",
       "95                           Reign  (2013‚Äì2017)   42 min     7.5     52,738\n",
       "96  A Series of Unfortunate Events  (2017‚Äì2019)   50 min     7.8     64,797\n",
       "97                  Criminal Minds     (2005‚Äì )   42 min     8.1    211,076\n",
       "98           Scream: The TV Series  (2015‚Äì2019)   45 min       7     43,908\n",
       "99      The Haunting of Hill House       (2018)  572 min     8.6    266,096\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "#Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "#A) Name,B) Year span,C) Genre,D) Run time,E) Ratings,F) Votes\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage https://github.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "\n",
    "#A) Name,B) Year span,C) Genre,D) Run time,E) Ratings,F) Votes\n",
    "nameData=[]\n",
    "yearData=[]\n",
    "runtimeData=[]\n",
    "ratingData=[]\n",
    "voteData=[]\n",
    "\n",
    "for i in range(1,101):\n",
    "    pth=\"/html/body/div[2]/div/div[2]/div/div[1]/div/div[3]/div[3]/div[\"+str(i)+\"]\"\n",
    "    nlist=driver.find_elements(By.XPATH, pth + \"/div[2]/h3/a\")\n",
    "    ylist=driver.find_elements(By.XPATH,pth + \"/div[2]/h3/span[2]\")\n",
    "    rtlist=driver.find_elements(By.XPATH,pth + \"/div[2]/p[1]/span[3]\")\n",
    "    ralist=driver.find_elements(By.XPATH,pth + \"/div[2]/div[1]/div[1]/span[2]\")\n",
    "    vlist=driver.find_elements(By.XPATH,pth + \"/div[2]/p[4]/span[2]\")\n",
    "    \n",
    "\n",
    "    for k in nlist:\n",
    "        nameData.append(k.text)\n",
    "    \n",
    "    for l in ylist:\n",
    "        yearData.append(l.text)\n",
    "                                 \n",
    "    for m in rtlist:\n",
    "        runtimeData.append(m.text)\n",
    "    \n",
    "    for n in ralist:\n",
    "        ratingData.append(n.text)     \n",
    "\n",
    "    for o in vlist:\n",
    "        voteData.append(o.text)\n",
    "\n",
    "#print(nameData)\n",
    "#print(yearData)\n",
    "#print(runtimeData)\n",
    "#print(ratingData)\n",
    "#print(voteData)\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data. A) Name,B) Year span,C) Genre,D) Run time,E) Ratings,F) Votes\n",
    "dfTvData= pd.DataFrame({'Name':nameData[:100],'Year span':yearData[:100],'Run time':runtimeData[:100], 'Ratings':ratingData[:100], 'Votes':voteData[:100]})\n",
    "dfTvData\n",
    "\n",
    "#close browser\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "13c749c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset name</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Task</th>\n",
       "      <th>No of instances</th>\n",
       "      <th>No of attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>150 Instances</td>\n",
       "      <td>4 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>303 Instances</td>\n",
       "      <td>13 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>13.61K Instances</td>\n",
       "      <td>16 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>20 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>178 Instances</td>\n",
       "      <td>13 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>569 Instances</td>\n",
       "      <td>30 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Car Evaluation</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>1.73K Instances</td>\n",
       "      <td>6 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>3.81K Instances</td>\n",
       "      <td>8 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>8.12K Instances</td>\n",
       "      <td>22 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>4.18K Instances</td>\n",
       "      <td>8 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wine Quality</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>4.9K Instances</td>\n",
       "      <td>12 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>286 Instances</td>\n",
       "      <td>9 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bank Marketing</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>45.21K Instances</td>\n",
       "      <td>17 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Statlog (German Credit Data)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>1K Instances</td>\n",
       "      <td>20 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Census Income</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Automobile</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>205 Instances</td>\n",
       "      <td>25 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Glass Identification</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>214 Instances</td>\n",
       "      <td>9 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Breast Cancer Wisconsin (Original)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>699 Instances</td>\n",
       "      <td>9 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Auto MPG</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>398 Instances</td>\n",
       "      <td>7 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Thyroid Disease</td>\n",
       "      <td>Multivariate, Domain-Theory</td>\n",
       "      <td>Classification</td>\n",
       "      <td>7.2K Instances</td>\n",
       "      <td>5 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Predict students' dropout and academic success</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>4.42K Instances</td>\n",
       "      <td>36 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Spambase</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>4.6K Instances</td>\n",
       "      <td>57 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Student Performance</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>649 Instances</td>\n",
       "      <td>33 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Credit Approval</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>690 Instances</td>\n",
       "      <td>15 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Optical Recognition of Handwritten Digits</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>5.62K Instances</td>\n",
       "      <td>64 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Online Retail</td>\n",
       "      <td>Multivariate, Sequential, Time-Series</td>\n",
       "      <td>Classification, Clustering</td>\n",
       "      <td>541.91K Instances</td>\n",
       "      <td>8 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Zoo</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>101 Instances</td>\n",
       "      <td>16 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Liver Disorders</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>345 Instances</td>\n",
       "      <td>5 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lung Cancer</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>32 Instances</td>\n",
       "      <td>56 Features</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Dataset name  \\\n",
       "0                                             Iris   \n",
       "1                                    Heart Disease   \n",
       "2                                            Adult   \n",
       "3                                 Dry Bean Dataset   \n",
       "4                                         Diabetes   \n",
       "5                                             Wine   \n",
       "6             Breast Cancer Wisconsin (Diagnostic)   \n",
       "7                                   Car Evaluation   \n",
       "8                       Rice (Cammeo and Osmancik)   \n",
       "9                                         Mushroom   \n",
       "10                                         Abalone   \n",
       "11                                    Wine Quality   \n",
       "12                                   Breast Cancer   \n",
       "13                                  Bank Marketing   \n",
       "14                    Statlog (German Credit Data)   \n",
       "15                                   Census Income   \n",
       "16                                      Automobile   \n",
       "17                            Glass Identification   \n",
       "18              Breast Cancer Wisconsin (Original)   \n",
       "19                                        Auto MPG   \n",
       "20                                 Thyroid Disease   \n",
       "21  Predict students' dropout and academic success   \n",
       "22                                        Spambase   \n",
       "23                             Student Performance   \n",
       "24                                 Credit Approval   \n",
       "25       Optical Recognition of Handwritten Digits   \n",
       "26                                   Online Retail   \n",
       "27                                             Zoo   \n",
       "28                                 Liver Disorders   \n",
       "29                                     Lung Cancer   \n",
       "\n",
       "                                Data type                        Task  \\\n",
       "0                                 Tabular              Classification   \n",
       "1                            Multivariate              Classification   \n",
       "2                            Multivariate              Classification   \n",
       "3                            Multivariate              Classification   \n",
       "4                                                                       \n",
       "5                                 Tabular              Classification   \n",
       "6                            Multivariate              Classification   \n",
       "7                            Multivariate              Classification   \n",
       "8                            Multivariate              Classification   \n",
       "9                            Multivariate              Classification   \n",
       "10                                Tabular  Classification, Regression   \n",
       "11                           Multivariate  Classification, Regression   \n",
       "12                           Multivariate              Classification   \n",
       "13                           Multivariate              Classification   \n",
       "14                           Multivariate              Classification   \n",
       "15                           Multivariate              Classification   \n",
       "16                           Multivariate                  Regression   \n",
       "17                           Multivariate              Classification   \n",
       "18                           Multivariate              Classification   \n",
       "19                           Multivariate                  Regression   \n",
       "20            Multivariate, Domain-Theory              Classification   \n",
       "21                                Tabular              Classification   \n",
       "22                           Multivariate              Classification   \n",
       "23                           Multivariate  Classification, Regression   \n",
       "24                           Multivariate              Classification   \n",
       "25                           Multivariate              Classification   \n",
       "26  Multivariate, Sequential, Time-Series  Classification, Clustering   \n",
       "27                           Multivariate              Classification   \n",
       "28                           Multivariate                  Regression   \n",
       "29                           Multivariate              Classification   \n",
       "\n",
       "      No of instances No of attribute  \n",
       "0       150 Instances      4 Features  \n",
       "1       303 Instances     13 Features  \n",
       "2    48.84K Instances     14 Features  \n",
       "3    13.61K Instances     16 Features  \n",
       "4                         20 Features  \n",
       "5       178 Instances     13 Features  \n",
       "6       569 Instances     30 Features  \n",
       "7     1.73K Instances      6 Features  \n",
       "8     3.81K Instances      8 Features  \n",
       "9     8.12K Instances     22 Features  \n",
       "10    4.18K Instances      8 Features  \n",
       "11     4.9K Instances     12 Features  \n",
       "12      286 Instances      9 Features  \n",
       "13   45.21K Instances     17 Features  \n",
       "14       1K Instances     20 Features  \n",
       "15   48.84K Instances     14 Features  \n",
       "16      205 Instances     25 Features  \n",
       "17      214 Instances      9 Features  \n",
       "18      699 Instances      9 Features  \n",
       "19      398 Instances      7 Features  \n",
       "20     7.2K Instances      5 Features  \n",
       "21    4.42K Instances     36 Features  \n",
       "22     4.6K Instances     57 Features  \n",
       "23      649 Instances     33 Features  \n",
       "24      690 Instances     15 Features  \n",
       "25    5.62K Instances     64 Features  \n",
       "26  541.91K Instances      8 Features  \n",
       "27      101 Instances     16 Features  \n",
       "28      345 Instances      5 Features  \n",
       "29       32 Instances     56 Features  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8. Details of Datasets from UCI machine learning repositories.\n",
    "#Url = https://archive.ics.uci.edu/\n",
    "#You have to find the following details:\n",
    "#A) Dataset name B) Data type C) Task D) Attribute type E) No of instances F) No of attribute G) Year\n",
    "#Note: - from the home page you have to go to the Show All Dataset page through code.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage \n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "\n",
    "dataset_btn = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a\")\n",
    "dataset_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#A) Dataset name B) Data type C) Task D) Attribute type E) No of instances F) No of attribute G) Year\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/h2/a\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div/div[2]/h2/a\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[5]/div/div[2]/h2/a\n",
    "                \n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/div/div[1]/span\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div/div[2]/div/div[1]/span\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[5]/div/div[2]/div/div[1]/span\n",
    "\n",
    "nameData=[]\n",
    "taskData=[]\n",
    "datatypeData=[]\n",
    "instanceData=[]\n",
    "noofattributeData=[]\n",
    "attributeData=[]\n",
    "yearData=[]\n",
    "\n",
    "page=1\n",
    "noofPages=3\n",
    "\n",
    "while page<=noofPages: # Travel thru pages.\n",
    "    \n",
    "    i=1\n",
    "    while i < 21 : # get datasets and info.\n",
    "        \n",
    "        pth=\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[\"+str(i)+\"]\"\n",
    "        namelist = driver.find_elements(By.XPATH,pth+\"/div[1]/div[2]/h2/a\")\n",
    "        tasklist = driver.find_elements(By.XPATH,pth+\"/div[1]/div[2]/div/div[1]/span\")\n",
    "        datatypelist = driver.find_elements(By.XPATH,pth+\"/div[1]/div[2]/div/div[2]/span\")\n",
    "        instancelist = driver.find_elements(By.XPATH, pth +\"/div[1]/div[2]/div/div[3]/span\")\n",
    "        noofattributelist = driver.find_elements(By.XPATH, pth +\"/div[1]/div[2]/div/div[4]/span\")\n",
    "        # I AM NOT GETTING ATTRIBUTE LIST AND DATE. NEED TO CLICK ON ARROW. I WROTE HERE THINKING THAT IT WILL FETCH DATA DUE TO\n",
    "        # XPATH. BUT IT FAILS AS IT IS NO VISIBLE ON PAGE. SO THESE LINES ARE NOT WORKING.\n",
    "        attributelist = driver.find_elements(By.XPATH, pth +\"/div[2]/div/table/tbody/tr/td[2]\")\n",
    "        yearlist = driver.find_elements(By.XPATH, pth +\"/div[2]/div/table/tbody/tr/td[3]\")\n",
    "    \n",
    "        for k in namelist:\n",
    "            nameData.append(k.text)\n",
    "    \n",
    "        for l in tasklist:\n",
    "            taskData.append(l.text)\n",
    "                                 \n",
    "        for m in datatypelist:\n",
    "            datatypeData.append(m.text)\n",
    "    \n",
    "        for n in instancelist:\n",
    "            instanceData.append(n.text)     \n",
    "\n",
    "        for o in noofattributelist:\n",
    "            noofattributeData.append(o.text)\n",
    "    \n",
    "        # THERE IS DIFFERENCE IN SVG TAG SEE BELOW XPATH.\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div[1]/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div[1]/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[5]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[7]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[9]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[11]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[13]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[15]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[17]/div/div[2]/svg\n",
    "        #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[19]/div/div[2]/svg\n",
    "        #I TRIED BELOW LOGIC.TO CLICK ON ARROW TO GET SOME DATA, HOWEVER THIS CODE NOT WORKING DUE TO TIMEOUT. I TRIED MULTIPLE WAYS BUT NO LUCK.\n",
    "        # TRIED CODE IS REMOVED FROM HERE.\n",
    "        # I AM NOT GETTING ATTRIBUTE LIST AND DATE.\n",
    "        #if i == 3:\n",
    "        #    arrow_btn = driver.find_element(By.XPATH,pth+\"/div[1]/div[2]/svg\")\n",
    "        #else:\n",
    "        #    arrow_btn = driver.find_element(By.XPATH,pth+\"/div/div[2]/svg\")\n",
    "        #driver.implicitly_wait(2) # seconds\n",
    "        #arrow_btn.click()\n",
    "       \n",
    "        #for p in attributelist:\n",
    "        #    attributeData.append(p.text)\n",
    "     \n",
    "        #for q in yearlist:\n",
    "        #    yearData.append(q.text)\n",
    "    \n",
    "        i += 2 #inner while for 10 dataset.\n",
    "    \n",
    "    \n",
    "    #nextButton=/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]\n",
    "    WebDriverWait(driver,30).until(expected_conditions.visibility_of_element_located((By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]\")))\n",
    "    nextButton=driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", nextButton)\n",
    "    \n",
    "    nextButton.click()\n",
    "    \n",
    "    # set implicit wait time\n",
    "    driver.implicitly_wait(3) # seconds\n",
    "    page += 1\n",
    "    \n",
    "#print(nameData)\n",
    "#print(taskData)\n",
    "#print(datatypeData)\n",
    "#print(instanceData)\n",
    "#print(noofattributeData)\n",
    "\n",
    "#print(attributeData) # Data is not collected due to some issue @SVG tag. used above.\n",
    "#print(yearData) # Data is not collected due to some issue @SVG tag. used above\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data. A) Dataset name B) Data type C) Task D) Attribute type E) No of instances F) No of attribute G) Year\n",
    "dfDataSetData= pd.DataFrame({'Dataset name':nameData[:30],'Data type':datatypeData[:30],'Task':taskData[:30], 'No of instances':instanceData[:30], 'No of attribute':noofattributeData[:30]})\n",
    "dfDataSetData\n",
    "\n",
    "#close browser\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BELOW CODE IS TO GET ARROW CLICK AND DATA BUT IT IS NOT WORKING.\n",
    "\n",
    "#this code is written to get the arrow click and get attribute details. however there are errors.\n",
    "#due to lack of time i wont be able to continue.\n",
    "\n",
    "#8. Details of Datasets from UCI machine learning repositories.\n",
    "#Url = https://archive.ics.uci.edu/\n",
    "#You have to find the following details:\n",
    "#A) Dataset name B) Data type C) Task D) Attribute type E) No of instances F) No of attribute G) Year\n",
    "#Note: - from the home page you have to go to the Show All Dataset page through code.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. First get the webpage https://github.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "\n",
    "dataset_btn = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a\")\n",
    "dataset_btn.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/h2/a\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div/div[2]/h2/a\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[5]/div/div[2]/h2/a\n",
    "                \n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/div/div[1]/span\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div/div[2]/div/div[1]/span\n",
    "#                /html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[5]/div/div[2]/div/div[1]/span\n",
    "\n",
    "nameData=[]\n",
    "taskData=[]\n",
    "datatypeData=[]\n",
    "instanceData=[]\n",
    "noofattributeData=[]\n",
    "attributeData=[]\n",
    "yearData=[]\n",
    "\n",
    "i=1\n",
    "\n",
    "pth=\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[\"+str(i)+\"]\"\n",
    "namelist = driver.find_elements(By.XPATH,pth+\"/div[1]/div[2]/h2/a\")\n",
    "tasklist = driver.find_elements(By.XPATH,pth+\"/div[1]/div[2]/div/div[1]/span\")\n",
    "datatypelist = driver.find_elements(By.XPATH,pth+\"/div[1]/div[2]/div/div[2]/span\")\n",
    "instancelist = driver.find_elements(By.XPATH, pth +\"/div[1]/div[2]/div/div[3]/span\")\n",
    "noofattributelist = driver.find_elements(By.XPATH, pth +\"/div[1]/div[2]/div/div[4]/span\")\n",
    "\n",
    "    #if i == 3:\n",
    "strXPATH = \"//*[name()='svg']//*[name()='path']\"\n",
    "#strXPATH = \"//*[name()='svg']//*[name()='path']\"\n",
    "#arrow_btn =  driver.find_element(By.XPATH, \"//div[local-name()='svg']/[localname()='path']\");\n",
    "#arrow_btn =  driver.find_element(By.XPATH, \"//div[local-name()='svg']/[localname()='path']\");\n",
    "\n",
    "#WebDriverWait(driver,10).until(expected_conditions.element_to_be_clickable((By.XPATH,strXPATH)))\n",
    "#arrow_btn = driver.find_element(By.XPATH,strXPATH)\n",
    "# set implicit wait time\n",
    "#WebDriverWait(driver,10).until(expected_conditions.element_to_be_clickable((By.XPATH,strXPATH)))\n",
    "#driver.implicitly_wait(30) # seconds\n",
    "#arrow_btn.click()\n",
    "\n",
    "attempts = 0\n",
    "while attempts < 2:\n",
    "    try:\n",
    "        arrow_btn = driver.find_element(By.XPATH,strXPATH)\n",
    "       #WebDriverWait(driver,1).until(expected_conditions.element_to_be_clickable((By.XPATH,strXPATH)))\n",
    "        print(attempts)\n",
    "        driver.implicitly_wait(1)\n",
    "        arrow_btn.click()\n",
    "        \n",
    "        #driver.implicitly_wait(30) # seconds\n",
    "        break\n",
    "    except exceptions as e:\n",
    "        #exceptions.StaleElementReferenceException e\n",
    "        print(e.exceptions)    \n",
    "    attempts += 1\n",
    "\n",
    "\n",
    "\n",
    "#Actions actionBuilder = new Actions(driver)\n",
    "#actionBuilder.click(arrow_btn).build().perform()\n",
    "\n",
    "#arrow_btn = driver.find_element(By.XPATH,pth+\"/div[1]/div[2]/svg\")\n",
    "    #else:\n",
    "    #    arrow_btn = driver.find_element(By.XPATH,pth+\"/div/div[2]/svg\")\n",
    "#driver.implicitly_wait(2) # seconds\n",
    "\n",
    "\n",
    "attributelist = driver.find_elements(By.XPATH, pth +\"/div[2]/div/table/tbody/tr/td[2]\")\n",
    "yearlist = driver.find_elements(By.XPATH, pth +\"/div[2]/div/table/tbody/tr/td[3]\")\n",
    "    \n",
    "    \n",
    "    \n",
    "for k in namelist:\n",
    "    nameData.append(k.text)\n",
    "    \n",
    "for l in tasklist:\n",
    "    taskData.append(l.text)\n",
    "                                 \n",
    "for m in datatypelist:\n",
    "    datatypeData.append(m.text)\n",
    "    \n",
    "for n in instancelist:\n",
    "    instanceData.append(n.text)     \n",
    "\n",
    "for o in noofattributelist:\n",
    "    noofattributeData.append(o.text)\n",
    "    \n",
    "    \n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div[1]/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[3]/div[1]/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[5]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[7]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[9]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[11]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[13]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[15]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[17]/div/div[2]/svg\n",
    "    #/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[19]/div/div[2]/svg\n",
    "    \n",
    "\n",
    "       \n",
    "for p in attributelist:\n",
    "    attributeData.append(p.text)\n",
    "     \n",
    "for q in yearlist:\n",
    "    yearData.append(q.text)\n",
    "    \n",
    "i += 2\n",
    "    \n",
    "print(nameData)\n",
    "print(taskData)\n",
    "print(datatypeData)\n",
    "print(instanceData)\n",
    "print(noofattributeData)\n",
    "print(attributeData)\n",
    "print(yearData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
