{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "168c4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (4.11.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (4.0.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\kumassuv\\appdata\\roaming\\python\\python39\\site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "#Install the selenium library\n",
    "!pip install -U selenium\n",
    "!pip install -U webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "b284d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project Coordinator (Data Analyst)</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Java/Python</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>BOYEN HADDIN CONSULTING AND TECHNOL...</td>\n",
       "      <td>3 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring For Data Analyst</td>\n",
       "      <td>Bangalore+14Hubli, Singapore, Oman, Qatar, Pat...</td>\n",
       "      <td>Kavya Staffing Solutions</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Ara Resources Private Limited</td>\n",
       "      <td>2 to 5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore+9Kurung Kumey, Chennai, Noida, Hyder...</td>\n",
       "      <td>Ashutosh Sabhashankar Chaturvedi Hi...</td>\n",
       "      <td>7 to 12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst Urgent Recruiment</td>\n",
       "      <td>Bangalore+14Bidar, Belgaum, Mangalore, Hubli, ...</td>\n",
       "      <td>Divya Interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apply Now   a Data Analyst</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>DEUGLO INFOSYSTEM PRIVATE LIMITED</td>\n",
       "      <td>1 to 2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apply Now   Data Analyst</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>DEUGLO INFOSYSTEM PRIVATE LIMITED</td>\n",
       "      <td>1 to 2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Needed for   Data Analyst</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>DEUGLO INFOSYSTEM PRIVATE LIMITED</td>\n",
       "      <td>1 to 2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Urgently need a Data Analyst</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>DEUGLO INFOSYSTEM PRIVATE LIMITED</td>\n",
       "      <td>1 to 2 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Job Title  \\\n",
       "0  Project Coordinator (Data Analyst)   \n",
       "1          Data Analyst - Java/Python   \n",
       "2             Hiring For Data Analyst   \n",
       "3                 Senior Data Analyst   \n",
       "4                        Data Analyst   \n",
       "5      Data Analyst Urgent Recruiment   \n",
       "6          Apply Now   a Data Analyst   \n",
       "7            Apply Now   Data Analyst   \n",
       "8           Needed for   Data Analyst   \n",
       "9        Urgently need a Data Analyst   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                          Bangalore   \n",
       "1                                          Bangalore   \n",
       "2  Bangalore+14Hubli, Singapore, Oman, Qatar, Pat...   \n",
       "3                                          Bangalore   \n",
       "4  Bangalore+9Kurung Kumey, Chennai, Noida, Hyder...   \n",
       "5  Bangalore+14Bidar, Belgaum, Mangalore, Hubli, ...   \n",
       "6                                          Bangalore   \n",
       "7                                          Bangalore   \n",
       "8                                          Bangalore   \n",
       "9                                          Bangalore   \n",
       "\n",
       "                             Company Name   Experience  \n",
       "0                     FUTURES AND CAREERS   2 to 4 Yrs  \n",
       "1  BOYEN HADDIN CONSULTING AND TECHNOL...   3 to 6 Yrs  \n",
       "2                Kavya Staffing Solutions   0 to 4 Yrs  \n",
       "3           Ara Resources Private Limited   2 to 5 Yrs  \n",
       "4  Ashutosh Sabhashankar Chaturvedi Hi...  7 to 12 Yrs  \n",
       "5                       Divya Interprises   0 to 4 Yrs  \n",
       "6       DEUGLO INFOSYSTEM PRIVATE LIMITED   1 to 2 Yrs  \n",
       "7       DEUGLO INFOSYSTEM PRIVATE LIMITED   1 to 2 Yrs  \n",
       "8       DEUGLO INFOSYSTEM PRIVATE LIMITED   1 to 2 Yrs  \n",
       "9       DEUGLO INFOSYSTEM PRIVATE LIMITED   1 to 2 Yrs  "
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1: Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. \n",
    "# You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "# This task will be done in following steps:\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "#2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "#3. Then click the search button.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.shine.com\"\n",
    "driver.get(url)\n",
    "\n",
    "search_job = driver.find_element(By.CLASS_NAME,\"input\")\n",
    "#search_job.send_keys('Data Analyst',Keys.RETURN)\n",
    "search_job.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "search_job1 = driver.find_element(By.ID,\"id_q\")\n",
    "search_job1.send_keys('Data Analyst')\n",
    "\n",
    "search_loc = driver.find_element(By.ID,\"id_loc\")\n",
    "#search_loc.send_keys('Bangalore',Keys.RETURN)\n",
    "search_loc.send_keys('Bangalore')\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#3. Then click the search button.\n",
    "search_btn = driver.find_element(By.CLASS_NAME,\"btn.btn-secondary.undefined\")\n",
    "#search_btn.click()\n",
    "search_btn.submit()\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "headData=[]\n",
    "jobExp=[]\n",
    "locData=[]\n",
    "cmpnyData=[]\n",
    "\n",
    "#4. Then scrape the data for the jobs results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#get poistion and url to get details.\n",
    "for i in soup.find_all('a',attrs={'href': re.compile(\"/jobs/\")}):\n",
    "    headData.append(i.text)\n",
    "\n",
    "# get number of experience.\n",
    "for i in soup.find_all('div',class_=\"jobCard_jobCard_cName__mYnow\"):\n",
    "    cmpnyData.append(i.text)     \n",
    "    \n",
    "# get number of experience.\n",
    "for i in soup.find_all('div',class_=\"jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"):\n",
    "    locData.append(i.text)    \n",
    "    \n",
    "# get number of experience.\n",
    "for i in soup.find_all('div',class_=\"jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"):\n",
    "    jobExp.append(i.text)\n",
    "\n",
    "#print data\n",
    "#print(headData,\"\\n\",linkData,\"\\n\",jobExp)\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "dfHeadNews= pd.DataFrame({'Job Title':headData[:10],'Location':locData[:10],'Company Name':cmpnyData[:10], 'Experience':jobExp[:10]})\n",
    "dfHeadNews\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0babbf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Bangalore+17Canada, Singapore, Oman, Qatar, Bh...</td>\n",
       "      <td>Kavya Staffing Solutions</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Bangalore+17Canada, Singapore, Oman, Qatar, Sh...</td>\n",
       "      <td>Kavya Staffing Solutions</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Skyleaf Consultants</td>\n",
       "      <td>5 to 10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist Urgent Recruitment</td>\n",
       "      <td>Bangalore+14Bidar, Gulbarga, Oman, Qatar, Coim...</td>\n",
       "      <td>Divya Interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Required for Data Scientist</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>DEUGLO INFOSYSTEM PRIVATE LIMITED</td>\n",
       "      <td>4 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Urgently need  Data Scientist</td>\n",
       "      <td>Bangalore+8Chennai, Noida, Hyderabad, Gurugram...</td>\n",
       "      <td>DEUGLO INFOSYSTEM PRIVATE LIMITED</td>\n",
       "      <td>4 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data scientist Bangalore</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Seven Geomax Consulting Private Lim...</td>\n",
       "      <td>6 to 9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Employberry Consultants Hiring For ...</td>\n",
       "      <td>3 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Bangalore+15Mangalore, Singapore, Oman, Qatar,...</td>\n",
       "      <td>Niharika Enterprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Needed for the post   Data Scientist</td>\n",
       "      <td>Bangalore+8Noida, Chennai, Hyderabad, Kolkata,...</td>\n",
       "      <td>DEUGLO INFOSYSTEM PRIVATE LIMITED</td>\n",
       "      <td>4 to 6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Job Title  \\\n",
       "0             Hiring For Data Scientist   \n",
       "1             Hiring For Data Scientist   \n",
       "2                        Data Scientist   \n",
       "3     Data Scientist Urgent Recruitment   \n",
       "4           Required for Data Scientist   \n",
       "5         Urgently need  Data Scientist   \n",
       "6              Data scientist Bangalore   \n",
       "7                        Data Scientist   \n",
       "8             Hiring For Data Scientist   \n",
       "9  Needed for the post   Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Bangalore+17Canada, Singapore, Oman, Qatar, Bh...   \n",
       "1  Bangalore+17Canada, Singapore, Oman, Qatar, Sh...   \n",
       "2                                          Bangalore   \n",
       "3  Bangalore+14Bidar, Gulbarga, Oman, Qatar, Coim...   \n",
       "4                                          Bangalore   \n",
       "5  Bangalore+8Chennai, Noida, Hyderabad, Gurugram...   \n",
       "6                                          Bangalore   \n",
       "7                                          Bangalore   \n",
       "8  Bangalore+15Mangalore, Singapore, Oman, Qatar,...   \n",
       "9  Bangalore+8Noida, Chennai, Hyderabad, Kolkata,...   \n",
       "\n",
       "                             Company Name   Experience  \n",
       "0                Kavya Staffing Solutions   0 to 4 Yrs  \n",
       "1                Kavya Staffing Solutions   0 to 4 Yrs  \n",
       "2                     Skyleaf Consultants  5 to 10 Yrs  \n",
       "3                       Divya Interprises   0 to 4 Yrs  \n",
       "4       DEUGLO INFOSYSTEM PRIVATE LIMITED   4 to 6 Yrs  \n",
       "5       DEUGLO INFOSYSTEM PRIVATE LIMITED   4 to 6 Yrs  \n",
       "6  Seven Geomax Consulting Private Lim...   6 to 9 Yrs  \n",
       "7  Employberry Consultants Hiring For ...   3 to 6 Yrs  \n",
       "8                    Niharika Enterprises   0 to 4 Yrs  \n",
       "9       DEUGLO INFOSYSTEM PRIVATE LIMITED   4 to 6 Yrs  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. \n",
    "#You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "#2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "#3. Then click the search button.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.shine.com\"\n",
    "driver.get(url)\n",
    "\n",
    "search_job = driver.find_element(By.CLASS_NAME,\"input\")\n",
    "#search_job.send_keys('Data Analyst',Keys.RETURN)\n",
    "search_job.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "search_job1 = driver.find_element(By.ID,\"id_q\")\n",
    "search_job1.send_keys('Data Scientist')\n",
    "\n",
    "search_loc = driver.find_element(By.ID,\"id_loc\")\n",
    "#search_loc.send_keys('Bangalore',Keys.RETURN)\n",
    "search_loc.send_keys('Bangalore')\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#3. Then click the search button.\n",
    "search_btn = driver.find_element(By.CLASS_NAME,\"btn.btn-secondary.undefined\")\n",
    "#search_btn.click()\n",
    "search_btn.submit()\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "headData=[]\n",
    "jobExp=[]\n",
    "locData=[]\n",
    "cmpnyData=[]\n",
    "#linkData=[]\n",
    "\n",
    "#4. Then scrape the data for the jobs results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#get poistion and url to get details.\n",
    "for i in soup.find_all('a',attrs={'href': re.compile(\"/jobs/\")}):\n",
    "    headData.append(i.text)\n",
    "  #linkData.append(i.get('href')) \n",
    "\n",
    "# get number of experience.\n",
    "for i in soup.find_all('div',class_=\"jobCard_jobCard_cName__mYnow\"):\n",
    "    cmpnyData.append(i.text)     \n",
    "    \n",
    "# get number of experience.\n",
    "for i in soup.find_all('div',class_=\"jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"):\n",
    "    locData.append(i.text)    \n",
    "    \n",
    "# get number of experience.\n",
    "for i in soup.find_all('div',class_=\"jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"):\n",
    "    jobExp.append(i.text)\n",
    "\n",
    "#print data\n",
    "#print(headData,\"\\n\",linkData,\"\\n\",jobExp)\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "dfHeadNews= pd.DataFrame({'Job Title':headData[:10],'Location':locData[:10],'Company Name':cmpnyData[:10], 'Experience':jobExp[:10]})\n",
    "dfHeadNews\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6754aa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Riding Glasses, UV Protection Clubmaster, Wayf...</td>\n",
       "      <td>‚Çπ379‚Çπ1,99981% off</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDEE</td>\n",
       "      <td>Gradient Round Sunglasses (51)</td>\n",
       "      <td>‚Çπ2,680</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>‚Çπ204‚Çπ1,29984% off</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (54)</td>\n",
       "      <td>‚Çπ239‚Çπ1,59985% off</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>‚Çπ179‚Çπ59970% off</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>‚Çπ599‚Çπ1,99970% off</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>Polarized, UV Protection Retro Square Sunglass...</td>\n",
       "      <td>‚Çπ429‚Çπ99957% off</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized, UV Protection Retro Square Sunglass...</td>\n",
       "      <td>‚Çπ379‚Çπ1,49974% off</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Rich Club</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ629‚Çπ89930% off</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>Night Vision, UV Protection, Polarized Wrap-ar...</td>\n",
       "      <td>‚Çπ199‚Çπ99980% off</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                                        Description  \\\n",
       "0   Singco India  Riding Glasses, UV Protection Clubmaster, Wayf...   \n",
       "1           IDEE                     Gradient Round Sunglasses (51)   \n",
       "2           SRPM             UV Protection Wayfarer Sunglasses (50)   \n",
       "3         PIRASO           UV Protection Clubmaster Sunglasses (54)   \n",
       "4      Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...   \n",
       "..           ...                                                ...   \n",
       "95      Fastrack                UV Protection Round Sunglasses (53)   \n",
       "96  Silver Kartz  Polarized, UV Protection Retro Square Sunglass...   \n",
       "97     ROYAL SON  Polarized, UV Protection Retro Square Sunglass...   \n",
       "98     Rich Club  UV Protection Retro Square Sunglasses (Free Size)   \n",
       "99     Elligator  Night Vision, UV Protection, Polarized Wrap-ar...   \n",
       "\n",
       "                Price Discount  \n",
       "0   ‚Çπ379‚Çπ1,99981% off  81% off  \n",
       "1              ‚Çπ2,680  84% off  \n",
       "2   ‚Çπ204‚Çπ1,29984% off  85% off  \n",
       "3   ‚Çπ239‚Çπ1,59985% off  70% off  \n",
       "4     ‚Çπ179‚Çπ59970% off  70% off  \n",
       "..                ...      ...  \n",
       "95  ‚Çπ599‚Çπ1,99970% off  50% off  \n",
       "96    ‚Çπ429‚Çπ99957% off  87% off  \n",
       "97  ‚Çπ379‚Çπ1,49974% off  79% off  \n",
       "98    ‚Çπ629‚Çπ89930% off  74% off  \n",
       "99    ‚Çπ199‚Çπ99980% off  71% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using XPATH\n",
    "\n",
    "#Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#6. Brand 7. Product Description 8. Price The attributes which you have to scrape is ticked marked in the below image.\n",
    "#To scrape the data you have to go through following steps:\n",
    "#1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "#2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and click the search icon\n",
    "#3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "#4. After scraping data from the first page, go to the ‚ÄúNext‚Äù Button at the bottom other page , then click on it.\n",
    "#5. Now scrape data from this page as usual\n",
    "#6. Repeat this until you get data for 100sunglasses.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "#2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and click the search icon\n",
    "search_prod = driver.find_element(By.CLASS_NAME,\"_3704LK\")   #Pke_EE \n",
    "search_prod.send_keys('sunglasses')\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"L0Z3Pu\") # #_2iLD__\n",
    "search.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#list for Brand, Product_Description, Price, offer\n",
    "brandData=[]\n",
    "priceData=[]\n",
    "prodData=[]\n",
    "offerData=[]\n",
    "\n",
    "#Get data for Brand, Product_Description, Price, offer using XPATH\n",
    "for i in range(3):\n",
    "    brandName=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    descProd=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    priceD =driver.find_elements(By.XPATH,\"//div[@class='_25b18c']\")\n",
    "    discountD=driver.find_elements(By.XPATH,\"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j in brandName:\n",
    "        brandData.append(j.text)\n",
    "\n",
    "    for k in descProd:\n",
    "        prodData.append(k.text)\n",
    "\n",
    "    for l in priceD:\n",
    "        priceData.append(l.text)\n",
    "        \n",
    "    for t in discountD:\n",
    "        offerData.append(t.text)\n",
    "    \n",
    "#print data to check\n",
    "#print(len(brandData),\"\\n\",len(prodData),\"\\n\",len(priceData),\"\\n\",len(offerData))\n",
    "#print(brandData[:100],\"\\n\",prodData[:100],\"\\n\",priceData[:100],\"\\n\",offerData[:100])\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "dfProducts= pd.DataFrame({'Brand':brandData[:100],'Description':prodData[:100],'Price':priceData[:100], 'Discount':offerData[:100]})\n",
    "dfProducts\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6575fc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Riding Glasses, UV Protection Clubmaster, Wayf...</td>\n",
       "      <td>‚Çπ379</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (55)</td>\n",
       "      <td>‚Çπ502</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>‚Çπ204</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (54)</td>\n",
       "      <td>‚Çπ239</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>‚Çπ179</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Aviator, Wayfarer Sunglasses (54)</td>\n",
       "      <td>‚Çπ599</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>‚Çπ499</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ379</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SmartGlass</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ219</td>\n",
       "      <td>39% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ199</td>\n",
       "      <td>30% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                        Description Price  \\\n",
       "0     Singco India  Riding Glasses, UV Protection Clubmaster, Wayf...  ‚Çπ379   \n",
       "1     Singco India          UV Protection Rectangular Sunglasses (55)  ‚Çπ502   \n",
       "2             SRPM             UV Protection Wayfarer Sunglasses (50)  ‚Çπ204   \n",
       "3           PIRASO           UV Protection Clubmaster Sunglasses (54)  ‚Çπ239   \n",
       "4        Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ‚Çπ179   \n",
       "..             ...                                                ...   ...   \n",
       "95       Elligator    UV Protection Aviator, Wayfarer Sunglasses (54)  ‚Çπ599   \n",
       "96  ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...  ‚Çπ499   \n",
       "97    Silver Kartz      UV Protection Wayfarer Sunglasses (Free Size)  ‚Çπ379   \n",
       "98      SmartGlass       UV Protection Aviator Sunglasses (Free Size)  ‚Çπ219   \n",
       "99        Fastrack       UV Protection Aviator Sunglasses (Free Size)  ‚Çπ199   \n",
       "\n",
       "   Discount  \n",
       "0   81% off  \n",
       "1   74% off  \n",
       "2   84% off  \n",
       "3   85% off  \n",
       "4   70% off  \n",
       "..      ...  \n",
       "95  74% off  \n",
       "96  78% off  \n",
       "97  80% off  \n",
       "98  39% off  \n",
       "99  30% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using BeautifulSoup\n",
    "\n",
    "#Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#6. Brand 7. Product Description 8. Price The attributes which you have to scrape is ticked marked in the below image.\n",
    "#To scrape the data you have to go through following steps:\n",
    "#1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "#2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and click the search icon\n",
    "#3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "#4. After scraping data from the first page, go to the ‚ÄúNext‚Äù Button at the bottom other page , then click on it.\n",
    "#5. Now scrape data from this page as usual\n",
    "#6. Repeat this until you get data for 100sunglasses.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and click the search icon\n",
    "search_prod = driver.find_element(By.CLASS_NAME,\"_3704LK\") #OR #Pke_EE \n",
    "search_prod.send_keys('sunglasses')\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"L0Z3Pu\") #OR #_2iLD__\n",
    "search.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#list for Brand, Product_Description, Price, offer\n",
    "brandData=[]\n",
    "priceData=[]\n",
    "prodData=[]\n",
    "offerData=[]\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "#4. Then scrape the data for the jobs results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "# Click on page to get 100 with loop\n",
    "while len(brandData) <= 100:\n",
    "    # get brandData.\n",
    "    for i in soup.find_all('div',class_=\"_2WkVRV\"):\n",
    "        brandData.append(i.text) \n",
    "\n",
    "    #get Product Description\n",
    "    for i in soup.find_all('a',class_=\"IRpwTa\"):\n",
    "        prodData.append(i.text)\n",
    "          \n",
    "    # get offer price.\n",
    "    for i in soup.find_all('div',class_=\"_30jeq3\"):\n",
    "        priceData.append(i.text)    \n",
    "\n",
    "    # get old price.\n",
    "    for i in soup.find_all('div',class_=\"_3I9_wc\"):\n",
    "        opriceData.append(i.text)    \n",
    "    \n",
    "    # get offer %\n",
    "    for i in soup.find_all('div',class_=\"_3Ay6Sb\"):\n",
    "        offerData.append(i.text)\n",
    "   \n",
    "    driver.implicitly_wait(3) # seconds\n",
    "    searchNxt=driver.find_element(By.XPATH,\"//a[@class='_1LKTO3']\")\n",
    "    # set implicit wait time\n",
    "    driver.implicitly_wait(3) # seconds\n",
    "    searchNxt.click()\n",
    "\n",
    "#print data to check\n",
    "#print(\"After page 2\\n\",len(brandData),\"\\n\",len(prodData),\"\\n\",len(priceData),\"\\n\",len(offerData))\n",
    "#print(brandData[:100],\"\\n\",prodData[:100],\"\\n\",priceData[:100],\"\\n\",offerData[:100])\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "dfProducts= pd.DataFrame({'Brand':brandData[:100],'Description':prodData[:100],'Price':priceData[:100], 'Discount':offerData[:100]})\n",
    "dfProducts\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ab6b7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review summary</th>\n",
       "      <th>Full review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Very very goodREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Photos superREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Value for money üòçREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Camera is awesomeBest battery backupA performe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>This is amazing at allREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Value for money üñ§üñ§READ MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>very good camera qualityREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>It‚Äôs very good battery life and display and vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NYCREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Damn this phone is a blast . Upgraded from and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating       Review summary  \\\n",
       "0       5             Terrific   \n",
       "1       5       Classy product   \n",
       "2       5    Terrific purchase   \n",
       "3       5       Classy product   \n",
       "4       5            Wonderful   \n",
       "..    ...                  ...   \n",
       "95      5    Terrific purchase   \n",
       "96      5            Brilliant   \n",
       "97      5            Fabulous!   \n",
       "98      5            Excellent   \n",
       "99      5  Best in the market!   \n",
       "\n",
       "                                          Full review  \n",
       "0                             Very very goodREAD MORE  \n",
       "1                               Photos superREAD MORE  \n",
       "2                          Value for money üòçREAD MORE  \n",
       "3   Camera is awesomeBest battery backupA performe...  \n",
       "4                     This is amazing at allREAD MORE  \n",
       "..                                                ...  \n",
       "95                        Value for money üñ§üñ§READ MORE  \n",
       "96                  very good camera qualityREAD MORE  \n",
       "97  It‚Äôs very good battery life and display and vi...  \n",
       "98                                       NYCREAD MORE  \n",
       "99  Damn this phone is a blast . Upgraded from and...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using XPATH and page navigation.\n",
    "\n",
    "#Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "#https://www.flipkart.com/apple-iphone-11-black-64-gb/product- reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market place=FLIPKART\n",
    "#As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "#1. Rating 2. Review summary 3. Full review\n",
    "# You have to scrape this data for first 100reviews\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "    #WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"_1LKTO3\")))\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "#1. Go to Flipkart webpage by url : https://www.flipkart.com/apple-iphone-11-black-64-gb/product- reviews\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "#list for 1. Rating 2. Review summary 3. Full review\n",
    "rateData=[]\n",
    "reviewData=[]\n",
    "fullreviewData=[]\n",
    "\n",
    "#Get new URL to fetch list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "#4. Then scrape the data for results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#Get data for 1. Rating 2. Review summary 3. Full review using XPATH\n",
    "\n",
    "\"\"\"\n",
    "#for loop with XPATH \n",
    "for i in range(3):\n",
    "    driver.implicitly_wait(5) # seconds   \n",
    "    rateD=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    reviewD=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    fullreviewD=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "\n",
    "    driver.implicitly_wait(5) # seconds\n",
    "    for j in rateD:\n",
    "        rateData.append(j.text)\n",
    "\n",
    "    driver.implicitly_wait(5) # seconds    \n",
    "    for k in reviewD:\n",
    "        reviewData.append(k.text)\n",
    "\n",
    "    driver.implicitly_wait(5) # seconds    \n",
    "    for l in fullreviewD:\n",
    "        fullreviewData.append(l.text)\n",
    "\n",
    "# for loop ends here\n",
    "\"\"\"\n",
    "\n",
    "#while loop to navigate page to get 100 reviews.\n",
    "m=1\n",
    "while len(rateData)<=100:\n",
    "\n",
    "    #get brandData.\n",
    "    for i in soup.find_all('div',class_=\"_3LWZlK _1BLPMq\"):\n",
    "        rateData.append(i.text) \n",
    "\n",
    "    #get Product Description\n",
    "    for i in soup.find_all('p',class_=\"_2-N8zT\"):\n",
    "        reviewData.append(i.text)\n",
    "          \n",
    "    #get offer price.\n",
    "    for i in soup.find_all('div',class_=\"t-ZTKy\"):\n",
    "        fullreviewData.append(i.text)    \n",
    "    \n",
    "    driver.implicitly_wait(5) # seconds\n",
    "    WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"_1LKTO3\")))\n",
    "    searchNxt=driver.find_element(By.XPATH,\"//a[@class='_1LKTO3']\")\n",
    "    \n",
    "    # set implicit wait time\n",
    "    WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"_1LKTO3\")))\n",
    "    driver.implicitly_wait(10) # seconds\n",
    "    searchNxt.click()\n",
    "    #searchNxt\n",
    "    \n",
    "    #Get new URL to fetch list\n",
    "    get_url = driver.current_url\n",
    "    #print(\"The current url is : \",m,\" : \" +str(get_url))\n",
    "\n",
    "    #4. Then scrape the data for results you get.\n",
    "    page=requests.get(get_url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    m=m+1   \n",
    "\n",
    "#print data to check\n",
    "#print(\"2nd Page \",len(rateData),\"\\n\",len(reviewData),\"\\n\",len(fullreviewData))\n",
    "#print(rateData[:100],\"\\n\",reviewData[:100],\"\\n\",fullreviewData[:100])\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "dfProductReview= pd.DataFrame({'Rating':rateData[:100],'Review summary':reviewData[:100],'Full review':fullreviewData[:100]})\n",
    "dfProductReview\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a71dac8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Modern Trendy Sneakers boot Sneakers Sneakers ...</td>\n",
       "      <td>‚Çπ369</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kardam&amp;sons</td>\n",
       "      <td>Fashionable Canvas Casual Partywear Outdoor Sn...</td>\n",
       "      <td>‚Çπ474</td>\n",
       "      <td>63% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>‚Çπ299</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Rebound LayUp SL Sneakers For Men</td>\n",
       "      <td>‚Çπ2,899</td>\n",
       "      <td>42% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sparx</td>\n",
       "      <td>SM-734 Sneakers For Men</td>\n",
       "      <td>‚Çπ718</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Nobelite</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ2,159</td>\n",
       "      <td>46% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>asian</td>\n",
       "      <td>Oxygen-01 white Running shoes dual capsule tec...</td>\n",
       "      <td>‚Çπ699</td>\n",
       "      <td>46% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>kardam&amp;sons</td>\n",
       "      <td>Fashionable Canvas Casual Partywear Outdoor Sn...</td>\n",
       "      <td>‚Çπ1,485</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Free Kicks</td>\n",
       "      <td>Combo Of 2 Shoes FK-444 &amp; FK-Capsule Sneakers ...</td>\n",
       "      <td>‚Çπ629</td>\n",
       "      <td>30% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Buzz Sneakers For Men</td>\n",
       "      <td>‚Çπ299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Brand                                        Description   Price  \\\n",
       "0      Magnolia  Modern Trendy Sneakers boot Sneakers Sneakers ...    ‚Çπ369   \n",
       "1   kardam&sons  Fashionable Canvas Casual Partywear Outdoor Sn...    ‚Çπ474   \n",
       "2        BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men    ‚Çπ299   \n",
       "3          PUMA                  Rebound LayUp SL Sneakers For Men  ‚Çπ2,899   \n",
       "4         Sparx                            SM-734 Sneakers For Men    ‚Çπ718   \n",
       "..          ...                                                ...     ...   \n",
       "95     Nobelite                                   Sneakers For Men  ‚Çπ2,159   \n",
       "96        asian  Oxygen-01 white Running shoes dual capsule tec...    ‚Çπ699   \n",
       "97  kardam&sons  Fashionable Canvas Casual Partywear Outdoor Sn...  ‚Çπ1,485   \n",
       "98   Free Kicks  Combo Of 2 Shoes FK-444 & FK-Capsule Sneakers ...    ‚Çπ629   \n",
       "99         PUMA                              Buzz Sneakers For Men    ‚Çπ299   \n",
       "\n",
       "   Discount  \n",
       "0   71% off  \n",
       "1   63% off  \n",
       "2   76% off  \n",
       "3   42% off  \n",
       "4   15% off  \n",
       "..      ...  \n",
       "95  46% off  \n",
       "96  46% off  \n",
       "97  54% off  \n",
       "98  30% off  \n",
       "99  70% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the search field.\n",
    "#You have to scrape 3 attributes of each sneaker:\n",
    "#1. Brand 2. Product Description 3. Price\n",
    "#As shown in the below image, you have to scrape the above attributes.\n",
    "\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and click the search icon\n",
    "search_prod = driver.find_element(By.CLASS_NAME,\"_3704LK\") #OR #Pke_EE \n",
    "search_prod.send_keys('sneakers')\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"L0Z3Pu\") #OR #_2iLD__\n",
    "search.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#list for Brand, Product_Description, Price, offer\n",
    "brandData=[]\n",
    "priceData=[]\n",
    "prodData=[]\n",
    "offerData=[]\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "#4. Then scrape the data for the jobs results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "# Click on page to get 100 with loop\n",
    "while len(brandData) <= 100:\n",
    "    # get brandData.\n",
    "    for i in soup.find_all('div',class_=\"_2WkVRV\"):\n",
    "        brandData.append(i.text) \n",
    "\n",
    "    #get Product Description\n",
    "    for i in soup.find_all('a',class_=\"IRpwTa\"):\n",
    "        prodData.append(i.text)\n",
    "          \n",
    "    # get offer price.\n",
    "    for i in soup.find_all('div',class_=\"_30jeq3\"):\n",
    "        priceData.append(i.text)    \n",
    "\n",
    "    # get old price.\n",
    "    for i in soup.find_all('div',class_=\"_3I9_wc\"):\n",
    "        opriceData.append(i.text)    \n",
    "    \n",
    "    # get offer %\n",
    "    for i in soup.find_all('div',class_=\"_3Ay6Sb\"):\n",
    "        offerData.append(i.text)\n",
    "   \n",
    "    driver.implicitly_wait(3) # seconds\n",
    "    searchNxt=driver.find_element(By.XPATH,\"//a[@class='_1LKTO3']\")\n",
    "    # set implicit wait time\n",
    "    driver.implicitly_wait(3) # seconds\n",
    "    searchNxt.click()\n",
    "\n",
    "#print data to check\n",
    "#print(\"After page 2\\n\",len(brandData),\"\\n\",len(prodData),\"\\n\",len(priceData),\"\\n\",len(offerData))\n",
    "#print(brandData[:100],\"\\n\",prodData[:100],\"\\n\",priceData[:100],\"\\n\",offerData[:100])\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "dfProducts= pd.DataFrame({'Brand':brandData[:100],'Description':prodData[:100],'Price':priceData[:100], 'Discount':offerData[:100]})\n",
    "dfProducts\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "f041b826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current url is:https://www.amazon.in/s?k=Laptop&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031&dc&ds=v1%3AkfcmKeQuVA1ON8p2cEPtKOy%2Fm3OiNl%2FP12ouoVclDdc&qid=1693111009&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_12\n",
      "0 \n",
      " 0 \n",
      " 0\n",
      "[] \n",
      " [] \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "#Q7: Go to webpage https://www.amazon.in/ Enter ‚ÄúLaptop‚Äù in the search field and \n",
    "#then click the search icon. Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù as shown in the below image\n",
    "#After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "#1. Title 2. Ratings 3. Price\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "#1. Go to webpage https://www.amazon.in/ \n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#2. Enter ‚ÄúLaptop‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and click the search icon\n",
    "search_prod = driver.find_element(By.ID,\"twotabsearchtextbox\") #OR #Pke_EE \n",
    "search_prod.send_keys('Laptop')\n",
    "\n",
    "search = driver.find_element(By.ID,\"nav-search-submit-button\") #OR #_2iLD__\n",
    "search.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(2) # seconds\n",
    "\n",
    "#3. select ‚ÄúIntel Core i7‚Äù‚Äù in the search \n",
    "searchCPU1=driver.find_element(By.XPATH,\"//li[@id='p_n_feature_thirteen_browse-bin/12598163031']/span[@class='a-list-item']/a\")\n",
    "WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.ID,\"p_n_feature_thirteen_browse-bin/12598163031\")))\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "searchCPU1.click()\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "#4. Then scrape the data for the jobs results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "# Get data for #1. Title 2. Ratings 3. Price\n",
    "headData=[]\n",
    "ratingData=[]\n",
    "priceData=[]\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "#get data using XPATH ---- but its not working.\n",
    "#\"//div[@id='search']/div[1]/div[1]/div/span[1]/div[1]/div[3]/div/div/div/div/div/div[2]/div/div/div[1]/h2/a/span/\"\n",
    "#\"//div[@id='search']/div[1]/div[1]/div/span[1]/div[1]/div[3]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[2]/a/span\"\n",
    "\n",
    "#Hname=driver.find_element(By.XPATH,\"//div[@id='search']/div[1]/div[1]/div/span[1]/div[1]/div[3]/div/div/div/div/div/div[2]/div/div/div[1]/h2/a/span/\")\n",
    "#WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.XPATH,\"//div[@id='search']/div[1]/div[1]/div/span[1]/div[1]/div[3]/div/div/div/div/div/div[2]/div/div/div[1]/h2/a/span/\")))\n",
    "#rateD=driver.find_element(By.XPATH,\"//div[@id='search']/div[1]/div[1]/div/span[1]/div[1]/div[3]/div/div/div/div/div/div[2]/div/div/div[2]/div/span[2]/a/span\")\n",
    "#WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.XPATH,\"//div[@id='search']/div[1]/div[1]/div/span[1]/div[1]/div[3]/div/div/div/div/div/div[2]/div/div/div[1]/h2/a/span/\")))\n",
    "#Hname=driver.find_element(By.XPATH,\"//h2[@class='a-size-mini.a-spacing-none.a-color-base.s-line-clamp-2']/a[@class='a-link-normal.s-underline-text.s-underline-link-text.s-link-style.a-text-normal']/span[@class='a-size-medium.a-color-base.a-text-normal']\")\n",
    "\n",
    "#for j in Hname:\n",
    "#     headData.append(j.text)\n",
    "#XPATH ends here.\n",
    "\n",
    "#get product Description\n",
    "for i in soup.find_all('span',class_=\"a-size-medium.a-color-base.a-text-normal\"):\n",
    "    headData.append(i.text) \n",
    "\n",
    "#get rating Description\n",
    "for i in soup.find_all('span',class_=\"a-size-base.s-underline-text\"):\n",
    "    ratingData.append(i.text)\n",
    "          \n",
    "# get price.\n",
    "for i in soup.find_all('span',class_=\"a-offscreen\"):\n",
    "    priceData.append(i.text)    \n",
    "\n",
    "#print data to check\n",
    "print(len(headData),\"\\n\",len(ratingData),\"\\n\",len(priceData))\n",
    "print(headData[:10],\"\\n\",ratingData[:10],\"\\n\",priceData[:10])\n",
    "\n",
    "#1. Title 2. Ratings 3. Price\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "#dfProducts= pd.DataFrame({'Title':headData[:100],'Ratings':ratingData[:100],'Price':priceData[:100]})\n",
    "#dfProducts\n",
    "\n",
    "#close browser\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "d2bc4bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>When the going gets weird, the weird turn pro.</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Music, Sports, Hunting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>When a train goes through a tunnel and it gets...</td>\n",
       "      <td>Corrie Ten Boom</td>\n",
       "      <td>Trust, Encouraging, Uplifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>If you think you are too small to make a diffe...</td>\n",
       "      <td>Dalai Lama</td>\n",
       "      <td>Inspirational, Funny, Change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>God doesn't require us to succeed, he only req...</td>\n",
       "      <td>Mother Teresa</td>\n",
       "      <td>Success, God, Mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Change your thoughts and you change your world.</td>\n",
       "      <td>Norman Vincent Peale</td>\n",
       "      <td>Inspirational, Motivational, Change</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Quote                Author  \\\n",
       "0   The essence of strategy is choosing what not t...        Michael Porter   \n",
       "1   One cannot and must not try to erase the past ...            Golda Meir   \n",
       "2   Patriotism means to stand by the country. It d...    Theodore Roosevelt   \n",
       "3   Death is something inevitable. When a man has ...        Nelson Mandela   \n",
       "4   You have to love a nation that celebrates its ...          Erma Bombeck   \n",
       "..                                                ...                   ...   \n",
       "95     When the going gets weird, the weird turn pro.    Hunter S. Thompson   \n",
       "96  When a train goes through a tunnel and it gets...       Corrie Ten Boom   \n",
       "97  If you think you are too small to make a diffe...            Dalai Lama   \n",
       "98  God doesn't require us to succeed, he only req...         Mother Teresa   \n",
       "99    Change your thoughts and you change your world.  Norman Vincent Peale   \n",
       "\n",
       "                                         Type  \n",
       "0   Essence, Deep Thought, Transcendentalism   \n",
       "1                  Inspiration, Past, Trying   \n",
       "2                        Country, Peace, War   \n",
       "3         Inspirational, Motivational, Death   \n",
       "4               4th Of July, Food, Patriotic   \n",
       "..                                        ...  \n",
       "95                    Music, Sports, Hunting   \n",
       "96             Trust, Encouraging, Uplifting   \n",
       "97              Inspirational, Funny, Change   \n",
       "98                      Success, God, Mother   \n",
       "99       Inspirational, Motivational, Change   \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "#The above task will be done in following steps:\n",
    "#1. First get the webpagehttps://www.azquotes.com/\n",
    "#2. Click on Top Quotes\n",
    "#3. Than scrap a) Quote b) Author c) Type Of Quotes\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "#1. Go to Flipkart webpage by url : https://www.azquotes.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.azquotes.com/\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#2. Click on Top Quotes\n",
    "search_top = driver.find_element(By.XPATH,\"//div[@id='menu']/div/div[3]/ul/li[5]/a\")\n",
    "search_top.click()\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "#4. Then scrape the data for the jobs results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#3. Than scrap a) Quote b) Author c) Type Of Quotes\n",
    "quoteData=[]\n",
    "authData=[]\n",
    "typeData=[]\n",
    "\n",
    "#get quote\n",
    "for i in soup.find_all('a',class_=\"title\"):\n",
    "    quoteData.append(i.text) \n",
    "    \n",
    "for i in soup.find_all('div',class_=\"author\"):\n",
    "    authData.append(i.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "for i in soup.find_all('div',class_=\"tags\"):\n",
    "    typeData.append(i.text.replace(\"\\n\",\"\"))\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "#print data\n",
    "#print(len(quoteData),\"\\n\",len(authData),\"\\n\",len(typeData))\n",
    "#print(quoteData[:100],\"\\n\",authData[:100],\"\\n\",typeData[:100])\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "#dfQuote=pd.DataFrame({'Quote':quoteData[:100],'Author':authData[:100],'Type':typeData[:100]})\n",
    "\n",
    "dfQuote=pd.DataFrame({})\n",
    "dfQuote['Quote']=quoteData[:100]\n",
    "dfQuote['Author']=authData[:100]\n",
    "dfQuote['Type']=typeData[:100]\n",
    "\n",
    "dfQuote\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "7f6b9761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current url is:https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\n",
      "0 \n",
      " 0 \n",
      " 0 \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "#Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.jagranjosh.com/\n",
    "#2. Then You have to click on the GK option\n",
    "#3. Then click on the List of all Prime Ministers of India\n",
    "#4. Then scrap the mentioned data and make theDataFrame.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "#list for PM name, date, office, remark\n",
    "pmName=[]\n",
    "dateData=[]\n",
    "officeData=[]\n",
    "remarkData=[]\n",
    "\n",
    "#1. Go to Flipkart webpage by url : https://www.azquotes.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#2. Then You have to click on the GK option\n",
    "search_top = driver.find_element(By.XPATH,\"//div[@id='__next']/header/nav/div/div/div[3]/ul/li[3]/a\")\n",
    "search_top.click()\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(3) # seconds\n",
    "\n",
    "#WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.ID,\"1611140963919\")))\n",
    "\n",
    "#3. Then click on the List of all Prime Ministers of India\n",
    "search_pm = driver.find_element(By.XPATH,\"//div[@id='1611140963919']/div[@id='popluarGK']/ul/li[2]/a\")\n",
    "WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.ID,\"popluarGK\")))\n",
    "\n",
    "driver.execute_script(\"arguments[0].click();\", search_pm)\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "#4. Then scrape the data for the jobs results you get.\n",
    "page=requests.get(get_url)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#Get data for Brand, Product_Description, Price, offer using XPATH\n",
    "#for i in range(3):\n",
    "WebDriverWait(driver,30).until(expected_conditions.presence_of_element_located((By.ID,\"itemdiv\")))\n",
    "\n",
    "#something is wrong code is not returning any data do not what's worng.\n",
    "pmList=driver.find_elements(By.XPATH,\"//div[@id='itemdiv']/div[4]/span/div[3]/table/tbody/tr[2]/td[2]/p/strong/a\")\n",
    "dateList=driver.find_elements(By.XPATH,\"//div[@id='itemdiv']/div[4]/span/div[3]/table/tbody/tr[2]/td[3]/p\")\n",
    "officeList=driver.find_elements(By.XPATH,\"//div[@id='itemdiv']/div[4]/span/div[3]/table/tbody/tr[2]/td[4]/p[1]/span\")\n",
    "remarkList=driver.find_elements(By.XPATH,\"//div[@id='itemdiv']/div[4]/span/div[3]/table/tbody/tr[2]/td[5]/p\")\n",
    "    \n",
    "for j in pmList:\n",
    "    pmName.append(j.text)\n",
    "\n",
    "for k in dateList:\n",
    "    dateData.append(k.text)\n",
    "    \n",
    "for l in officeList:\n",
    "    officeData.append(l.text)\n",
    "\n",
    "for m in remarkList:\n",
    "    remarkData.append(m.text)\n",
    "    \n",
    "#print data to check\n",
    "print(len(pmName),\"\\n\",len(dateData),\"\\n\",len(officeData),\"\\n\",len(remarkData))\n",
    "#print(pmName[:100],\"\\n\",dateData[:100],\"\\n\",officeData[:100],\"\\n\",remarkData[:100])\n",
    "\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "#dfPmList= pd.DataFrame({'Brand':brandData[:10],'Description':prodData[:10],'Price':priceData[:10], 'Discount':offerData[:10]})\n",
    "#dfPmList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "1b90d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['By Dan Mihalascu'] \n",
      " ['JAN 11 2023'] \n",
      " ['Half Of US New Car Buyers Believe EVs Are Too Expensive: Survey'] \n",
      " ['Seven in 10 respondents said they expect their next vehicle to cost less than $50,000, which would leave out most EVs on sale today.']\n"
     ]
    }
   ],
   "source": [
    "#Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.motor1.com/\n",
    "#2. Then You have to type in the search bar ‚Äô50 most expensive cars‚Äô\n",
    "#3. Then click on 50 most expensive cars in the world..\n",
    "#4. Then scrap the mentioned data and make the dataframe.\n",
    "\n",
    "#import required libraray.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "#1. First get the webpage https://www.motor1.com/\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.motor1.com/\"\n",
    "driver.get(url)\n",
    "#driver.maximize_window()\n",
    "\n",
    "#2. Then You have to type in the search bar ‚Äô50 most expensive cars‚Äô\n",
    "searchText = driver.find_element(By.ID,\"search_input\")\n",
    "searchText.send_keys('50 most expensive cars')\n",
    "\n",
    "#3. Then click on 50 most expensive cars in the world..\n",
    "searchCar = driver.find_element(By.CLASS_NAME,\"m1-search-panel-button.m1-search-form-button-animate.icon-search-svg\")\n",
    "searchCar.click()\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(10) # seconds\n",
    "\n",
    "#Get new URL to fetch job list\n",
    "get_url = driver.current_url\n",
    "#print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "#list for car data\n",
    "authName=[] #span class=label\n",
    "dateData=[] #span class=date\n",
    "commentData=[] #div text-box\n",
    "remarkData=[] #a text\n",
    "\n",
    "#get data using XPATH\n",
    "for i in range(2):\n",
    "    authList=driver.find_elements(By.XPATH,\"//div[@id='page_index_articles_search']/div[9]/div/div[1]/div/div/div[1]/div/div[2]/span[1]/span\")\n",
    "    dateList=driver.find_elements(By.XPATH,\"//div[@id='page_index_articles_search']/div[9]/div/div[1]/div/div/div[1]/div/div[2]/span[2]\")\n",
    "    comList=driver.find_elements(By.XPATH,\"//div[@id='page_index_articles_search']/div[9]/div/div[1]/div/div/div[1]/div/div[1]/h3/a\")\n",
    "    remList=driver.find_elements(By.XPATH,\"//div[@id='page_index_articles_search']/div[9]/div/div[1]/div/div/div[1]/div/div[1]/a\")\n",
    "\n",
    "for j in authList:\n",
    "    authName.append(j.text)\n",
    "\n",
    "for k in dateList:\n",
    "    dateData.append(k.text)\n",
    "\n",
    "for l in comList:\n",
    "    commentData.append(l.text)\n",
    "\n",
    "for m in remList:\n",
    "    remarkData.append(m.text)\n",
    "\n",
    "# set implicit wait time\n",
    "driver.implicitly_wait(5) # seconds\n",
    "\n",
    "#print data\n",
    "#print(len(authName),\"\\n\",len(dateData),\"\\n\",len(commentData),\"\\n\",len(remarkData))\n",
    "print(authName,\"\\n\",dateData,\"\\n\",commentData,\"\\n\",remarkData)\n",
    "\n",
    "#dataframe not working\n",
    "#dfCar=pd.DataFrame({})\n",
    "#dfCar['Author']=authName\n",
    "#dfCar['Date']=dateData\n",
    "#dfCar['Comments']=commentData\n",
    "#dfCar['Remarks']=remarkData\n",
    "\n",
    "#dfCar\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1364b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
